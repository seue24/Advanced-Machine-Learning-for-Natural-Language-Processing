{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import kagglehub\n",
    "import spacy\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO dataset\n",
    "path = kagglehub.dataset_download(\"nagasai524/mini-coco2014-dataset-for-image-captioning\")\n",
    "\n",
    "with open(os.path.join(path, \"captions.json\"), \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    annotations = data[\"annotations\"] if isinstance(data, dict) else data\n",
    "\n",
    "captions = {}\n",
    "for item in annotations:\n",
    "    img_id = item[\"image_id\"]\n",
    "    if img_id not in captions:\n",
    "        captions[img_id] = []\n",
    "    captions[img_id].append(item[\"caption\"])\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    if any(f.endswith(\".jpg\") for f in files):\n",
    "        img_folder = root\n",
    "        break\n",
    "\n",
    "img_ids = list(captions.keys())\n",
    "print(f\"Ready: {len(captions)} images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image loading function\n",
    "def get_image(idx):\n",
    "    \"\"\"Load image at original size\"\"\"\n",
    "    img_id = img_ids[idx]\n",
    "    img_path = os.path.join(img_folder, f\"COCO_train2014_{img_id:012d}.jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        img_path = os.path.join(img_folder, f\"{img_id}.jpg\")\n",
    "    return Image.open(img_path).convert(\"RGB\"), captions[img_id]\n",
    "\n",
    "# Show test image\n",
    "img, caps = get_image(87)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Test Image (Index 87)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGround truth captions:\")\n",
    "for cap in caps:\n",
    "    print(f\"  - {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAT model\n",
    "import sys\n",
    "sys.path.insert(0, \"a-PyTorch-Tutorial-to-Image-Captioning\")  # Adjust path if needed\n",
    "from models import Encoder, Decoder\n",
    "\n",
    "checkpoint_path = \"checkpoint/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=str(device))\n",
    "\n",
    "sat_encoder = checkpoint[\"encoder\"].to(device).eval()\n",
    "sat_decoder = checkpoint[\"decoder\"].to(device).eval()\n",
    "\n",
    "with open(\"checkpoint/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json\", \"r\") as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "print(\"SAT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption with SAT (test if model works)\n",
    "def generate_sat_caption(idx, show_image=True):\n",
    "    \"\"\"Run SAT on one COCO image and print GT and predicted captions\"\"\"\n",
    "    img, gt_caps = get_image(idx)\n",
    "\n",
    "    if show_image:\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    # SAT preprocessing\n",
    "    img_array = np.array(img)\n",
    "    if len(img_array.shape) == 2:\n",
    "        img_array = np.stack([img_array] * 3, axis=2)\n",
    "    \n",
    "    img_resized = Image.fromarray(img_array).resize((256, 256))\n",
    "    img_array = np.array(img_resized).transpose(2, 0, 1) / 255.0\n",
    "    img_tensor = torch.FloatTensor(img_array).to(device)\n",
    "    \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "    img_tensor = normalize(img_tensor).unsqueeze(0)\n",
    "    \n",
    "    # Encode\n",
    "    encoder_out = sat_encoder(img_tensor)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)\n",
    "    \n",
    "    # Decode\n",
    "    h, c = sat_decoder.init_hidden_state(encoder_out)\n",
    "    \n",
    "    word_indices = [word_map[\"<start>\"]]\n",
    "    \n",
    "    for step in range(50):\n",
    "        prev_word = torch.LongTensor([word_indices[-1]]).to(device)\n",
    "        embeddings = sat_decoder.embedding(prev_word)\n",
    "        \n",
    "        awe, alpha = sat_decoder.attention(encoder_out, h)\n",
    "        gate = sat_decoder.sigmoid(sat_decoder.f_beta(h))\n",
    "        awe = gate * awe\n",
    "        \n",
    "        h, c = sat_decoder.decode_step(\n",
    "            torch.cat([embeddings, awe], dim=1), (h, c)\n",
    "        )\n",
    "        \n",
    "        scores = sat_decoder.fc(h)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "        next_word = scores.argmax(1).item()\n",
    "        \n",
    "        word_indices.append(next_word)\n",
    "        \n",
    "        if next_word == word_map[\"<end>\"]:\n",
    "            break\n",
    "    \n",
    "    # Get caption\n",
    "    words = [rev_word_map[idx] for idx in word_indices[1:-1]]\n",
    "    pred_caption = \" \".join(words)\n",
    "    \n",
    "    print(f\"SAT caption: {pred_caption}\")\n",
    "    \n",
    "    return img, gt_caps, pred_caption\n",
    "\n",
    "# Test on image 87\n",
    "for i in [87]:\n",
    "    generate_sat_caption(i, show_image=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy for POS tagging\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"spaCy loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SAT attention\n",
    "def extract_sat_attention(idx):\n",
    "    \"\"\"\n",
    "    Extract per-word cross-attention from SAT model\n",
    "    Returns: img, gt_caps, pred_caption, word_attentions [num_words, 24, 24], words\n",
    "    \"\"\"\n",
    "    img, gt_caps = get_image(idx)\n",
    "    \n",
    "    # SAT preprocessing\n",
    "    img_array = np.array(img)\n",
    "    if len(img_array.shape) == 2:\n",
    "        img_array = np.stack([img_array] * 3, axis=2)\n",
    "    \n",
    "    img_resized = Image.fromarray(img_array).resize((256, 256))\n",
    "    img_array = np.array(img_resized).transpose(2, 0, 1) / 255.0\n",
    "    img_tensor = torch.FloatTensor(img_array).to(device)\n",
    "    \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "    img_tensor = normalize(img_tensor).unsqueeze(0)\n",
    "    \n",
    "    # Encode\n",
    "    encoder_out = sat_encoder(img_tensor)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)\n",
    "    \n",
    "    # Decode with attention tracking\n",
    "    h, c = sat_decoder.init_hidden_state(encoder_out)\n",
    "    \n",
    "    word_indices = [word_map[\"<start>\"]]\n",
    "    alphas_list = []\n",
    "    \n",
    "    for step in range(50):\n",
    "        prev_word = torch.LongTensor([word_indices[-1]]).to(device)\n",
    "        embeddings = sat_decoder.embedding(prev_word)\n",
    "        \n",
    "        awe, alpha = sat_decoder.attention(encoder_out, h)\n",
    "        alpha = alpha.view(enc_image_size, enc_image_size)\n",
    "        \n",
    "        gate = sat_decoder.sigmoid(sat_decoder.f_beta(h))\n",
    "        awe = gate * awe\n",
    "        \n",
    "        h, c = sat_decoder.decode_step(\n",
    "            torch.cat([embeddings, awe], dim=1), (h, c)\n",
    "        )\n",
    "        \n",
    "        scores = sat_decoder.fc(h)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "        next_word = scores.argmax(1).item()\n",
    "        \n",
    "        word_indices.append(next_word)\n",
    "        alphas_list.append(alpha)\n",
    "        \n",
    "        if next_word == word_map[\"<end>\"]:\n",
    "            break\n",
    "    \n",
    "    # Remove start/end tokens\n",
    "    word_indices = word_indices[1:-1]\n",
    "    alphas = torch.stack(alphas_list[:-1])\n",
    "    \n",
    "    # Resize to 24x24\n",
    "    alphas_resized = F.interpolate(\n",
    "        alphas.unsqueeze(1), \n",
    "        size=(24, 24), \n",
    "        mode=\"bilinear\"\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    words = [rev_word_map[idx] for idx in word_indices]\n",
    "    pred_caption = \" \".join(words)\n",
    "    \n",
    "    print(f\"Caption: {pred_caption}\")\n",
    "    print(f\"Captured {len(alphas_resized)} words of attention\")\n",
    "    print(f\"Attention shape: {alphas_resized.shape}\")\n",
    "    \n",
    "    return img, gt_caps, pred_caption, alphas_resized, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation methods (6 total: 2 baselines + 4 novel)\n",
    "\n",
    "# Baseline 1: Sentence-based (sum across all words)\n",
    "def aggregate_sentence_based(word_attentions, k_percent=0.5):\n",
    "    \"\"\"Sum attention across all words\"\"\"\n",
    "    aggregated = word_attentions.mean(dim=0)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(aggregated.flatten(), k).values.min()\n",
    "    mask = (aggregated >= threshold).float()\n",
    "    \n",
    "    return aggregated.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Baseline 2: Word-based (union of per-word top-k)\n",
    "def aggregate_word_based(word_attentions, k_percent=0.5):\n",
    "    \"\"\"Select top-k pixels per word, then union\"\"\"\n",
    "    k_per_word = int(576 * k_percent / len(word_attentions))\n",
    "    \n",
    "    combined_mask = torch.zeros(24, 24)\n",
    "    for word_attn in word_attentions:\n",
    "        threshold = torch.topk(word_attn.flatten(), k_per_word).values.min()\n",
    "        combined_mask += (word_attn >= threshold).float()\n",
    "    \n",
    "    aggregated = word_attentions.mean(dim=0)\n",
    "    mask = (combined_mask > 0).float()\n",
    "    \n",
    "    return aggregated.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel 1: POS-weighted\n",
    "def aggregate_pos_weighted(word_attentions, words, k_percent=0.5):\n",
    "    \"\"\"Weight attention by part-of-speech importance\"\"\"\n",
    "    doc = nlp(\" \".join(words))\n",
    "    \n",
    "    weighted_attn = torch.zeros_like(word_attentions[0])\n",
    "    for i, (word, token) in enumerate(zip(words, doc)):\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            weight = 2.0\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            weight = 1.5\n",
    "        elif token.pos_ == \"ADJ\":\n",
    "            weight = 1.2\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        weighted_attn += word_attentions[i] * weight\n",
    "    \n",
    "    aggregated = weighted_attn / len(words)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(aggregated.flatten(), k).values.min()\n",
    "    mask = (aggregated >= threshold).float()\n",
    "    \n",
    "    return aggregated.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel 2: Temporal-decay\n",
    "def aggregate_temporal_decay(word_attentions, k_percent=0.5, decay=0.9):\n",
    "    \"\"\"Weight earlier words higher with exponential decay\"\"\"\n",
    "    weighted_attn = torch.zeros_like(word_attentions[0])\n",
    "    \n",
    "    for i, word_attn in enumerate(word_attentions):\n",
    "        weight = decay ** i\n",
    "        weighted_attn += word_attn * weight\n",
    "    \n",
    "    aggregated = weighted_attn / len(word_attentions)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(aggregated.flatten(), k).values.min()\n",
    "    mask = (aggregated >= threshold).float()\n",
    "    \n",
    "    return aggregated.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel 3: Variance-based\n",
    "def aggregate_variance_based(word_attentions, k_percent=0.5):\n",
    "    \"\"\"Select pixels with high variance across words\"\"\"\n",
    "    variance = word_attentions.var(dim=0)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(variance.flatten(), k).values.min()\n",
    "    mask = (variance >= threshold).float()\n",
    "    \n",
    "    return variance.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel 4: Entropy-based\n",
    "def aggregate_entropy_based(word_attentions, k_percent=0.5):\n",
    "    \"\"\"Select pixels with high entropy (uncertainty) across words\"\"\"\n",
    "    word_attentions_norm = torch.softmax(word_attentions.flatten(1), dim=1)\n",
    "    word_attentions_norm = word_attentions_norm.view_as(word_attentions)\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "    entropy = -(word_attentions_norm * torch.log(word_attentions_norm + epsilon)).sum(dim=0)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(entropy.flatten(), k).values.min()\n",
    "    mask = (entropy >= threshold).float()\n",
    "    \n",
    "    return entropy.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "print(\"All 6 aggregation methods loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def visualize_attention_overlay(img, pred_caption, agg_attn, mask, method_name, k_viz=0.3):\n",
    "    \"\"\"\n",
    "    Visualize attention with overlay\n",
    "    k_viz: percentage for visualization (30% for clarity)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Original\\n{pred_caption[:40]}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    img_array = np.array(img.resize((384, 384)))\n",
    "    attn_resized = np.array(Image.fromarray((agg_attn * 255).astype(np.uint8)).resize((384, 384)))\n",
    "    attn_resized = attn_resized / 255.0\n",
    "    \n",
    "    axes[1].imshow(img_array)\n",
    "    axes[1].imshow(attn_resized, cmap=\"hot\", alpha=0.5)\n",
    "    axes[1].set_title(f\"{method_name}\\nHeatmap Overlay\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    axes[2].imshow(agg_attn, cmap=\"hot\", interpolation=\"bilinear\")\n",
    "    axes[2].set_title(\"Attention Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    # Recompute mask for visualization\n",
    "    agg_tensor = torch.from_numpy(agg_attn) if isinstance(agg_attn, np.ndarray) else agg_attn\n",
    "    k_viz_pixels = int(576 * k_viz)\n",
    "    threshold_viz = torch.topk(agg_tensor.flatten(), k_viz_pixels).values.min()\n",
    "    mask_viz = (agg_tensor >= threshold_viz).float().numpy()\n",
    "    \n",
    "    mask_resized = np.array(Image.fromarray((mask_viz * 255).astype(np.uint8)).resize((384, 384), Image.NEAREST))\n",
    "    mask_resized = mask_resized / 255.0\n",
    "    \n",
    "    axes[3].imshow(img_array)\n",
    "    axes[3].imshow(mask_resized, cmap=\"Reds\", alpha=0.4)\n",
    "    axes[3].set_title(f\"Top {int(k_viz*100)}% Patches\\nSelected for Attack\")\n",
    "    axes[3].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function with adjustable k_percent\n",
    "def visualize_attention_overlay(img, pred_caption, agg_attn, mask, method_name=\"Sentence-based\", k_viz=0.3):\n",
    "    \"\"\"\n",
    "    Visualize attention with overlay on original image\n",
    "    k_viz: percentage for visualization (default 30% for clearer view)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Original\\n{pred_caption[:40]}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    img_array = np.array(img.resize((384, 384)))\n",
    "    attn_resized = np.array(Image.fromarray((agg_attn * 255).astype(np.uint8)).resize((384, 384)))\n",
    "    attn_resized = attn_resized / 255.0\n",
    "    \n",
    "    axes[1].imshow(img_array)\n",
    "    axes[1].imshow(attn_resized, cmap=\"hot\", alpha=0.5)\n",
    "    axes[1].set_title(f\"{method_name}\\nHeatmap Overlay\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    axes[2].imshow(agg_attn, cmap=\"hot\", interpolation=\"bilinear\")\n",
    "    axes[2].set_title(\"Attention Heatmap\\n(Smoothed)\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    # Recompute mask with k_viz for visualization\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        mask_tensor = torch.from_numpy(mask)\n",
    "        agg_tensor = torch.from_numpy(agg_attn)\n",
    "    else:\n",
    "        mask_tensor = mask\n",
    "        agg_tensor = agg_attn\n",
    "    \n",
    "    k_viz_pixels = int(576 * k_viz)\n",
    "    threshold_viz = torch.topk(agg_tensor.flatten(), k_viz_pixels).values.min()\n",
    "    mask_viz = (agg_tensor >= threshold_viz).float().numpy()\n",
    "    \n",
    "    mask_resized = np.array(Image.fromarray((mask_viz * 255).astype(np.uint8)).resize((384, 384), Image.NEAREST))\n",
    "    mask_resized = mask_resized / 255.0\n",
    "    \n",
    "    axes[3].imshow(img_array)\n",
    "    axes[3].imshow(mask_resized, cmap=\"Reds\", alpha=0.4)\n",
    "    axes[3].set_title(f\"Top {int(k_viz*100)}% Patches\\nSelected for Attack\")\n",
    "    axes[3].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test extraction\n",
    "idx = 87\n",
    "img, gt_caps, pred_caption, word_attns, words = extract_sat_attention(idx)\n",
    "\n",
    "print(f\"\\nWords: {words}\")\n",
    "print(f\"Attention shape: {word_attns.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all 6 methods on image 87\n",
    "idx = 87\n",
    "img, gt_caps, pred_caption, word_attns, words = extract_sat_attention(idx)\n",
    "\n",
    "methods = {\n",
    "    \"Sentence-based\": lambda: aggregate_sentence_based(word_attns, k_percent=0.5),\n",
    "    \"Word-based\": lambda: aggregate_word_based(word_attns, k_percent=0.5),\n",
    "    \"POS-weighted\": lambda: aggregate_pos_weighted(word_attns, words, k_percent=0.5),\n",
    "    \"Temporal-decay\": lambda: aggregate_temporal_decay(word_attns, k_percent=0.5),\n",
    "    \"Variance-based\": lambda: aggregate_variance_based(word_attns, k_percent=0.5),\n",
    "    \"Entropy-based\": lambda: aggregate_entropy_based(word_attns, k_percent=0.5)\n",
    "}\n",
    "\n",
    "for name, method in methods.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    agg, mask = method()\n",
    "    visualize_attention_overlay(img, pred_caption, agg, mask, name, k_viz=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
