{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport torch\nimport json\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport kagglehub\nimport spacy\nfrom scipy.stats import pearsonr\nfrom scipy.spatial.distance import cosine\n\nnp.random.seed(42)\ntorch.manual_seed(42)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO dataset\n",
    "path = kagglehub.dataset_download(\"nagasai524/mini-coco2014-dataset-for-image-captioning\")\n",
    "\n",
    "with open(os.path.join(path, \"captions.json\"), \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    annotations = data[\"annotations\"] if isinstance(data, dict) else data\n",
    "\n",
    "captions = {}\n",
    "for item in annotations:\n",
    "    img_id = item[\"image_id\"]\n",
    "    if img_id not in captions:\n",
    "        captions[img_id] = []\n",
    "    captions[img_id].append(item[\"caption\"])\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    if any(f.endswith(\".jpg\") for f in files):\n",
    "        img_folder = root\n",
    "        break\n",
    "\n",
    "img_ids = list(captions.keys())\n",
    "print(f\"Ready: {len(captions)} images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image loading function\n",
    "def get_image(idx):\n",
    "    \"\"\"Load and resize image to 384x384\"\"\"\n",
    "    img_id = img_ids[idx]\n",
    "    img_path = os.path.join(img_folder, f\"COCO_train2014_{img_id:012d}.jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        img_path = os.path.join(img_folder, f\"{img_id}.jpg\")\n",
    "    return Image.open(img_path).convert(\"RGB\").resize((384, 384)), captions[img_id]\n",
    "\n",
    "# Show selected image\n",
    "selected = [87]\n",
    "fig, axes = plt.subplots(1, len(selected), figsize=(8, 8))\n",
    "if len(selected) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, ax in zip(selected, axes):\n",
    "    img, caps = get_image(idx)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "for idx in selected:\n",
    "    img, caps = get_image(idx)\n",
    "    print(f\"\\nGround truth captions:\")\n",
    "    for cap in caps:\n",
    "        print(f\"  - {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.48145466, 0.4578275, 0.40821073], \n",
    "                        [0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "def to_tensor(idx):\n",
    "    img, _ = get_image(idx)\n",
    "    return normalize(img).unsqueeze(0).to(device)\n",
    "\n",
    "test = to_tensor(0)\n",
    "print(f\"Tensor ready for BLIP: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
    "model.eval()\n",
    "print(\"BLIP model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption baseline\n",
    "def generate_blip_caption(idx, show_image=True):\n",
    "    \"\"\"Run BLIP on one COCO image and print GT and predicted captions\"\"\"\n",
    "    img, gt_caps = get_image(idx)\n",
    "\n",
    "    if show_image:\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "\n",
    "    pred_caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"BLIP caption: {pred_caption}\")\n",
    "\n",
    "    return img, gt_caps, pred_caption\n",
    "\n",
    "for i in [87]:\n",
    "    generate_blip_caption(i, show_image=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention\n",
    "def extract_blip_attention(idx):\n",
    "    \"\"\"Extract vision attention from BLIP model\"\"\"\n",
    "    img, gt_caps = get_image(idx)\n",
    "    inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    vision_attentions = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        if len(output) > 1 and output[1] is not None:\n",
    "            vision_attentions.append(output[1].detach())\n",
    "    \n",
    "    hooks = []\n",
    "    for layer in model.vision_model.encoder.layers:\n",
    "        hook = layer.self_attn.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "        pred_caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    print(f\"Caption: {pred_caption}\")\n",
    "    print(f\"Captured {len(vision_attentions)} layers of attention\")\n",
    "    \n",
    "    return img, gt_caps, pred_caption, vision_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Aggregation methods\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept:\n    import os\n    os.system(\"python -m spacy download en_core_web_sm\")\n    nlp = spacy.load(\"en_core_web_sm\")\n\n# Sentence-based\ndef aggregate_sentence_based(vision_attentions, k_percent=0.5):\n    \"\"\"Aggregate attention across all layers (mean pooling)\"\"\"\n    all_attention = []\n    for layer_attn in vision_attentions:\n        attn = layer_attn.mean(dim=1)\n        cls_to_patches = attn[0, 0, 1:]\n        all_attention.append(cls_to_patches)\n    \n    aggregated = torch.stack(all_attention).mean(dim=0)\n    aggregated_2d = aggregated.reshape(24, 24)\n    \n    k = int(576 * k_percent)\n    threshold = torch.topk(aggregated.flatten(), k).values.min()\n    mask = (aggregated_2d >= threshold).float()\n    \n    return aggregated_2d.cpu().numpy(), mask.cpu().numpy()\n\n\n# Word-based (Stability-based)\ndef aggregate_word_based(vision_attentions, k_percent=0.5):\n    \"\"\"\n    Select patches with STABLE attention across layers.\n    Rationale: Patches corresponding to semantic objects/words should receive\n    consistent attention across layers, as opposed to spurious activations.\n    High stability = likely object regions = word-level semantics.\n    \"\"\"\n    all_attention = []\n    for layer_attn in vision_attentions:\n        attn = layer_attn.mean(dim=1)\n        cls_to_patches = attn[0, 0, 1:]\n        all_attention.append(cls_to_patches)\n    \n    all_attention = torch.stack(all_attention)  # [num_layers, 576]\n    \n    # Compute stability: inverse of coefficient of variation (std/mean)\n    # Higher stability = lower relative variance = more consistent attention\n    mean_attn = all_attention.mean(dim=0)\n    std_attn = all_attention.std(dim=0)\n    \n    # Avoid division by zero\n    epsilon = 1e-10\n    stability = mean_attn / (std_attn + epsilon)  # High stability = consistent patches\n    \n    # Weight by both stability AND attention magnitude\n    # This selects patches that are both important AND consistently attended\n    stability_weighted = stability * mean_attn\n    stability_2d = stability_weighted.reshape(24, 24)\n    \n    k = int(576 * k_percent)\n    threshold = torch.topk(stability_weighted.flatten(), k).values.min()\n    mask = (stability_2d >= threshold).float()\n    \n    return stability_2d.cpu().numpy(), mask.cpu().numpy()\n\n\n# POS-weighted\ndef aggregate_pos_weighted(vision_attentions, caption, k_percent=0.5):\n    \"\"\"\n    Weight attention by caption complexity (number of content words).\n    More content words = more semantic information = weight layers differently.\n    \"\"\"\n    doc = nlp(caption)\n    \n    # Count content words (nouns, verbs, adjectives)\n    content_words = sum(1 for token in doc if token.pos_ in [\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\"])\n    function_words = len(doc) - content_words\n    \n    # Complexity ratio: higher = more content-heavy caption\n    content_ratio = content_words / max(len(doc), 1)\n    \n    all_attention = []\n    for i, layer_attn in enumerate(vision_attentions):\n        attn = layer_attn.mean(dim=1)\n        cls_to_patches = attn[0, 0, 1:]\n        \n        # Weight middle layers more for content-heavy captions\n        # (middle layers in ViT capture semantic/object features)\n        layer_pos = i / len(vision_attentions)\n        middle_bias = 1.0 + content_ratio * (1.0 - 4 * (layer_pos - 0.5)**2)\n        \n        all_attention.append(cls_to_patches * middle_bias)\n    \n    aggregated = torch.stack(all_attention).mean(dim=0)\n    aggregated_2d = aggregated.reshape(24, 24)\n    \n    k = int(576 * k_percent)\n    threshold = torch.topk(aggregated.flatten(), k).values.min()\n    mask = (aggregated_2d >= threshold).float()\n    \n    return aggregated_2d.cpu().numpy(), mask.cpu().numpy()\n\n\n# Temporal-decay\ndef aggregate_temporal_decay(vision_attentions, k_percent=0.5, decay=0.95):\n    \"\"\"\n    Weight early layers higher (exponential decay for later layers).\n    Rationale: Early layers capture low-level features that may be more vulnerable.\n    \"\"\"\n    all_attention = []\n    \n    for i, layer_attn in enumerate(vision_attentions):\n        attn = layer_attn.mean(dim=1)\n        cls_to_patches = attn[0, 0, 1:]\n        weight = decay ** i\n        all_attention.append(cls_to_patches * weight)\n    \n    aggregated = torch.stack(all_attention).mean(dim=0)\n    aggregated_2d = aggregated.reshape(24, 24)\n    \n    k = int(576 * k_percent)\n    threshold = torch.topk(aggregated.flatten(), k).values.min()\n    mask = (aggregated_2d >= threshold).float()\n    \n    return aggregated_2d.cpu().numpy(), mask.cpu().numpy()\n\n\n# Variance-based\ndef aggregate_variance_based(vision_attentions, k_percent=0.5):\n    \"\"\"\n    Select patches with HIGH variance across layers.\n    Rationale: High variance = different layers attend differently = potentially\n    important features with layer-specific processing.\n    \"\"\"\n    all_attention = []\n    for layer_attn in vision_attentions:\n        attn = layer_attn.mean(dim=1)\n        cls_to_patches = attn[0, 0, 1:]\n        all_attention.append(cls_to_patches)\n    \n    all_attention = torch.stack(all_attention)\n    variance = all_attention.var(dim=0)\n    variance_2d = variance.reshape(24, 24)\n    \n    k = int(576 * k_percent)\n    threshold = torch.topk(variance.flatten(), k).values.min()\n    mask = (variance_2d >= threshold).float()\n    \n    return variance_2d.cpu().numpy(), mask.cpu().numpy()\n\n\n# Entropy-based\ndef aggregate_entropy_based(vision_attentions, k_percent=0.5):\n    \"\"\"\n    Select patches with HIGH entropy (uncertainty) across layers.\n    Rationale: High entropy = inconsistent attention distribution = uncertain regions\n    that might be critical decision boundaries.\n    \"\"\"\n    all_attention = []\n    for layer_attn in vision_attentions:\n        attn = layer_attn.mean(dim=1)\n        cls_to_patches = attn[0, 0, 1:]\n        all_attention.append(cls_to_patches)\n    \n    all_attention = torch.stack(all_attention)\n    \n    # Normalize each layer's attention to probability distribution\n    all_attention = torch.softmax(all_attention, dim=1)\n    \n    epsilon = 1e-10\n    entropy = -(all_attention * torch.log(all_attention + epsilon)).sum(dim=0)\n    entropy_2d = entropy.reshape(24, 24)\n    \n    k = int(576 * k_percent)\n    threshold = torch.topk(entropy.flatten(), k).values.min()\n    mask = (entropy_2d >= threshold).float()\n    \n    return entropy_2d.cpu().numpy(), mask.cpu().numpy()\n\nprint(\"All 6 aggregation methods loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function with adjustable k_percent\n",
    "def visualize_attention_overlay(img, pred_caption, agg_attn, mask, method_name=\"Sentence-based\", k_viz=0.3):\n",
    "    \"\"\"\n",
    "    Visualize attention with overlay on original image\n",
    "    k_viz: percentage for visualization (default 30% for clearer view)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Original\\n{pred_caption[:40]}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    img_array = np.array(img.resize((384, 384)))\n",
    "    attn_resized = np.array(Image.fromarray((agg_attn * 255).astype(np.uint8)).resize((384, 384)))\n",
    "    attn_resized = attn_resized / 255.0\n",
    "    \n",
    "    axes[1].imshow(img_array)\n",
    "    axes[1].imshow(attn_resized, cmap=\"hot\", alpha=0.5)\n",
    "    axes[1].set_title(f\"{method_name}\\nHeatmap Overlay\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    axes[2].imshow(agg_attn, cmap=\"hot\", interpolation=\"bilinear\")\n",
    "    axes[2].set_title(\"Attention Heatmap\\n(Smoothed)\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    # Recompute mask with k_viz for visualization\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        mask_tensor = torch.from_numpy(mask)\n",
    "        agg_tensor = torch.from_numpy(agg_attn)\n",
    "    else:\n",
    "        mask_tensor = mask\n",
    "        agg_tensor = agg_attn\n",
    "    \n",
    "    k_viz_pixels = int(576 * k_viz)\n",
    "    threshold_viz = torch.topk(agg_tensor.flatten(), k_viz_pixels).values.min()\n",
    "    mask_viz = (agg_tensor >= threshold_viz).float().numpy()\n",
    "    \n",
    "    mask_resized = np.array(Image.fromarray((mask_viz * 255).astype(np.uint8)).resize((384, 384), Image.NEAREST))\n",
    "    mask_resized = mask_resized / 255.0\n",
    "    \n",
    "    axes[3].imshow(img_array)\n",
    "    axes[3].imshow(mask_resized, cmap=\"Reds\", alpha=0.4)\n",
    "    axes[3].set_title(f\"Top {int(k_viz*100)}% Patches\\nSelected for Attack\")\n",
    "    axes[3].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Statistical analysis functions\ndef compute_attention_statistics(agg_attn, mask, method_name):\n    \"\"\"Compute statistical measures for attention maps\"\"\"\n    stats = {\n        'method': method_name,\n        'mean': np.mean(agg_attn),\n        'std': np.std(agg_attn),\n        'min': np.min(agg_attn),\n        'max': np.max(agg_attn),\n        'median': np.median(agg_attn),\n        'sparsity': np.sum(mask) / mask.size,  # Fraction of patches selected\n        'entropy': -np.sum(agg_attn * np.log(agg_attn + 1e-10))  # Attention entropy\n    }\n    return stats\n\ndef compute_mask_overlap(mask1, mask2):\n    \"\"\"Compute Intersection over Union (IoU) between two binary masks\"\"\"\n    intersection = np.sum(mask1 * mask2)\n    union = np.sum(np.maximum(mask1, mask2))\n    return intersection / (union + 1e-10)\n\ndef analyze_attention_diversity(methods_results):\n    \"\"\"\n    Analyze diversity between different aggregation methods\n    methods_results: dict of {method_name: (agg_attn, mask)}\n    \"\"\"\n    method_names = list(methods_results.keys())\n    n_methods = len(method_names)\n    \n    # Compute statistics for each method\n    print(\"=\" * 80)\n    print(\"ATTENTION STATISTICS PER METHOD\")\n    print(\"=\" * 80)\n    stats_df = []\n    for name, (agg, mask) in methods_results.items():\n        stats = compute_attention_statistics(agg, mask, name)\n        stats_df.append(stats)\n    \n    stats_df = pd.DataFrame(stats_df)\n    print(stats_df.to_string(index=False))\n    print()\n    \n    # Compute pairwise correlations between attention maps\n    print(\"=\" * 80)\n    print(\"PAIRWISE CORRELATIONS (Pearson) - Attention Maps\")\n    print(\"=\" * 80)\n    corr_matrix = np.zeros((n_methods, n_methods))\n    for i, name1 in enumerate(method_names):\n        for j, name2 in enumerate(method_names):\n            agg1, _ = methods_results[name1]\n            agg2, _ = methods_results[name2]\n            corr, _ = pearsonr(agg1.flatten(), agg2.flatten())\n            corr_matrix[i, j] = corr\n    \n    corr_df = pd.DataFrame(corr_matrix, \n                          index=method_names, \n                          columns=method_names)\n    print(corr_df.to_string())\n    print()\n    \n    # Compute pairwise IoU between binary masks\n    print(\"=\" * 80)\n    print(\"PAIRWISE IoU (Intersection over Union) - Binary Masks\")\n    print(\"=\" * 80)\n    iou_matrix = np.zeros((n_methods, n_methods))\n    for i, name1 in enumerate(method_names):\n        for j, name2 in enumerate(method_names):\n            _, mask1 = methods_results[name1]\n            _, mask2 = methods_results[name2]\n            iou = compute_mask_overlap(mask1, mask2)\n            iou_matrix[i, j] = iou\n    \n    iou_df = pd.DataFrame(iou_matrix, \n                         index=method_names, \n                         columns=method_names)\n    print(iou_df.to_string())\n    print()\n    \n    # Visualize correlation heatmap\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    im1 = axes[0].imshow(corr_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n    axes[0].set_xticks(range(n_methods))\n    axes[0].set_yticks(range(n_methods))\n    axes[0].set_xticklabels(method_names, rotation=45, ha='right')\n    axes[0].set_yticklabels(method_names)\n    axes[0].set_title('Correlation Matrix\\n(Attention Maps)')\n    \n    for i in range(n_methods):\n        for j in range(n_methods):\n            text = axes[0].text(j, i, f'{corr_matrix[i, j]:.2f}',\n                              ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n    \n    plt.colorbar(im1, ax=axes[0])\n    \n    im2 = axes[1].imshow(iou_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n    axes[1].set_xticks(range(n_methods))\n    axes[1].set_yticks(range(n_methods))\n    axes[1].set_xticklabels(method_names, rotation=45, ha='right')\n    axes[1].set_yticklabels(method_names)\n    axes[1].set_title('IoU Matrix\\n(Binary Masks)')\n    \n    for i in range(n_methods):\n        for j in range(n_methods):\n            text = axes[1].text(j, i, f'{iou_matrix[i, j]:.2f}',\n                              ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n    \n    plt.colorbar(im2, ax=axes[1])\n    plt.tight_layout()\n    plt.show()\n    \n    return stats_df, corr_df, iou_df\n\nprint(\"Statistical analysis functions loaded!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare all 6 methods on selected image\nidx = 87\nimg, gt_caps, pred_caption, vision_attn = extract_blip_attention(idx)\n\n# Generate attention with k=0.5 (50% for actual attack)\nmethods = {\n    \"Sentence-based\": lambda: aggregate_sentence_based(vision_attn, k_percent=0.5),\n    \"Word-based\": lambda: aggregate_word_based(vision_attn, k_percent=0.5),\n    \"POS-weighted\": lambda: aggregate_pos_weighted(vision_attn, pred_caption, k_percent=0.5),\n    \"Temporal-decay\": lambda: aggregate_temporal_decay(vision_attn, k_percent=0.5),\n    \"Variance-based\": lambda: aggregate_variance_based(vision_attn, k_percent=0.5),\n    \"Entropy-based\": lambda: aggregate_entropy_based(vision_attn, k_percent=0.5)\n}\n\n# Store results for analysis\nmethods_results = {}\n\n# Visualize with k=0.3 (30% for clearer view like paper)\nfor name, method in methods.items():\n    print(f\"\\n=== {name} ===\")\n    agg, mask = method()\n    methods_results[name] = (agg, mask)\n    visualize_attention_overlay(img, pred_caption, agg, mask, name, k_viz=0.3)\n\n# Perform statistical analysis\nprint(\"\\n\\n\")\nprint(\"#\" * 80)\nprint(\"QUANTITATIVE ANALYSIS OF ATTENTION METHODS\")\nprint(\"#\" * 80)\nstats_df, corr_df, iou_df = analyze_attention_diversity(methods_results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Multi-image analysis: Test diversity across different images\ntest_indices = [87, 0, 42, 100, 150]  # Sample of diverse images\n\nprint(\"=\" * 80)\nprint(\"MULTI-IMAGE ANALYSIS: Average Correlation and IoU Across Images\")\nprint(\"=\" * 80)\n\nall_correlations = []\nall_ious = []\n\nfor idx in test_indices:\n    print(f\"\\nProcessing image {idx}...\")\n    img, gt_caps, pred_caption, vision_attn = extract_blip_attention(idx)\n    \n    methods_results = {}\n    for name, method_fn in methods.items():\n        # Recreate lambdas with current vision_attn\n        if name == \"Sentence-based\":\n            agg, mask = aggregate_sentence_based(vision_attn, k_percent=0.5)\n        elif name == \"Word-based\":\n            agg, mask = aggregate_word_based(vision_attn, k_percent=0.5)\n        elif name == \"POS-weighted\":\n            agg, mask = aggregate_pos_weighted(vision_attn, pred_caption, k_percent=0.5)\n        elif name == \"Temporal-decay\":\n            agg, mask = aggregate_temporal_decay(vision_attn, k_percent=0.5)\n        elif name == \"Variance-based\":\n            agg, mask = aggregate_variance_based(vision_attn, k_percent=0.5)\n        elif name == \"Entropy-based\":\n            agg, mask = aggregate_entropy_based(vision_attn, k_percent=0.5)\n        \n        methods_results[name] = (agg, mask)\n    \n    # Compute correlations and IoU for this image\n    method_names = list(methods_results.keys())\n    n_methods = len(method_names)\n    \n    corr_matrix = np.zeros((n_methods, n_methods))\n    iou_matrix = np.zeros((n_methods, n_methods))\n    \n    for i, name1 in enumerate(method_names):\n        for j, name2 in enumerate(method_names):\n            agg1, mask1 = methods_results[name1]\n            agg2, mask2 = methods_results[name2]\n            \n            corr, _ = pearsonr(agg1.flatten(), agg2.flatten())\n            corr_matrix[i, j] = corr\n            \n            iou = compute_mask_overlap(mask1, mask2)\n            iou_matrix[i, j] = iou\n    \n    all_correlations.append(corr_matrix)\n    all_ious.append(iou_matrix)\n\n# Average across all images\navg_corr = np.mean(all_correlations, axis=0)\navg_iou = np.mean(all_ious, axis=0)\nstd_corr = np.std(all_correlations, axis=0)\nstd_iou = np.std(all_ious, axis=0)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"AVERAGE CORRELATION ACROSS IMAGES\")\nprint(\"=\" * 80)\ncorr_df_avg = pd.DataFrame(avg_corr, index=method_names, columns=method_names)\nprint(corr_df_avg.to_string())\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"AVERAGE IoU ACROSS IMAGES\")\nprint(\"=\" * 80)\niou_df_avg = pd.DataFrame(avg_iou, index=method_names, columns=method_names)\nprint(iou_df_avg.to_string())\n\n# Visualize average correlations\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nim1 = axes[0].imshow(avg_corr, cmap='RdYlGn', vmin=0, vmax=1)\naxes[0].set_xticks(range(n_methods))\naxes[0].set_yticks(range(n_methods))\naxes[0].set_xticklabels(method_names, rotation=45, ha='right')\naxes[0].set_yticklabels(method_names)\naxes[0].set_title(f'Average Correlation\\n({len(test_indices)} images)')\n\nfor i in range(n_methods):\n    for j in range(n_methods):\n        text = axes[0].text(j, i, f'{avg_corr[i, j]:.2f}',\n                          ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n\nplt.colorbar(im1, ax=axes[0])\n\nim2 = axes[1].imshow(avg_iou, cmap='RdYlGn', vmin=0, vmax=1)\naxes[1].set_xticks(range(n_methods))\naxes[1].set_yticks(range(n_methods))\naxes[1].set_xticklabels(method_names, rotation=45, ha='right')\naxes[1].set_yticklabels(method_names)\naxes[1].set_title(f'Average IoU\\n({len(test_indices)} images)')\n\nfor i in range(n_methods):\n    for j in range(n_methods):\n        text = axes[1].text(j, i, f'{avg_iou[i, j]:.2f}',\n                          ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n\nplt.colorbar(im2, ax=axes[1])\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"KEY INSIGHTS\")\nprint(\"=\" * 80)\nprint(\"Lower correlation/IoU = More diverse attention selection\")\nprint(\"Methods should show DIFFERENT selections to justify multiple approaches\")\nprint(\"\\nOff-diagonal values (comparing different methods):\")\nfor i in range(n_methods):\n    for j in range(i+1, n_methods):\n        print(f\"  {method_names[i]} vs {method_names[j]}: \"\n              f\"Corr={avg_corr[i,j]:.3f} (±{std_corr[i,j]:.3f}), \"\n              f\"IoU={avg_iou[i,j]:.3f} (±{std_iou[i,j]:.3f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all 6 methods on selected image\n",
    "idx = 87\n",
    "img, gt_caps, pred_caption, vision_attn = extract_blip_attention(idx)\n",
    "\n",
    "# Generate attention with k=0.5 (50% for actual attack)\n",
    "methods = {\n",
    "    \"Sentence-based\": lambda: aggregate_sentence_based(vision_attn, k_percent=0.5),\n",
    "    \"Word-based\": lambda: aggregate_word_based(vision_attn, k_percent=0.5),\n",
    "    \"POS-weighted\": lambda: aggregate_pos_weighted(vision_attn, pred_caption, k_percent=0.5),\n",
    "    \"Temporal-decay\": lambda: aggregate_temporal_decay(vision_attn, k_percent=0.5),\n",
    "    \"Variance-based\": lambda: aggregate_variance_based(vision_attn, k_percent=0.5),\n",
    "    \"Entropy-based\": lambda: aggregate_entropy_based(vision_attn, k_percent=0.5)\n",
    "}\n",
    "\n",
    "# But visualize with k=0.3 (30% for clearer view like paper)\n",
    "for name, method in methods.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    agg, mask = method()\n",
    "    visualize_attention_overlay(img, pred_caption, agg, mask, name, k_viz=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}