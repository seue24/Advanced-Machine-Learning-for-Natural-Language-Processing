{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import kagglehub\n",
    "import spacy\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO dataset\n",
    "path = kagglehub.dataset_download(\"nagasai524/mini-coco2014-dataset-for-image-captioning\")\n",
    "\n",
    "with open(os.path.join(path, \"captions.json\"), \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    annotations = data[\"annotations\"] if isinstance(data, dict) else data\n",
    "\n",
    "captions = {}\n",
    "for item in annotations:\n",
    "    img_id = item[\"image_id\"]\n",
    "    if img_id not in captions:\n",
    "        captions[img_id] = []\n",
    "    captions[img_id].append(item[\"caption\"])\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    if any(f.endswith(\".jpg\") for f in files):\n",
    "        img_folder = root\n",
    "        break\n",
    "\n",
    "img_ids = list(captions.keys())\n",
    "print(f\"Ready: {len(captions)} images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image loading function\n",
    "def get_image(idx):\n",
    "    \"\"\"Load image at original size\"\"\"\n",
    "    img_id = img_ids[idx]\n",
    "    img_path = os.path.join(img_folder, f\"COCO_train2014_{img_id:012d}.jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        img_path = os.path.join(img_folder, f\"{img_id}.jpg\")\n",
    "    return Image.open(img_path).convert(\"RGB\"), captions[img_id]\n",
    "\n",
    "# Show test image\n",
    "img, caps = get_image(87)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGround truth captions:\")\n",
    "for cap in caps:\n",
    "    print(f\"  - {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VIT-GPT2 model\n",
    "from transformers import AutoTokenizer, AutoModelForVision2Seq\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation methods (6 total: 2 baselines + 4 novel)\n",
    "\n",
    "# Baseline 1: Sentence-based (sum across all words)\n",
    "def aggregate_sentence_based(word_attentions, k_percent=0.5):\n",
    "    \"\"\"Sum attention across all words\"\"\"\n",
    "    aggregated = word_attentions.mean(dim=0)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(aggregated.flatten(), k).values.min()\n",
    "    mask = (aggregated >= threshold).float()\n",
    "    \n",
    "    return aggregated.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Baseline 2: Word-based (union of per-word top-k)\n",
    "def aggregate_word_based(word_attentions, k_percent=0.5):\n",
    "    \"\"\"Select top-k pixels per word, then union\"\"\"\n",
    "    k_per_word = int(576 * k_percent / len(word_attentions))\n",
    "    \n",
    "    combined_mask = torch.zeros(24, 24)\n",
    "    for word_attn in word_attentions:\n",
    "        threshold = torch.topk(word_attn.flatten(), k_per_word).values.min()\n",
    "        combined_mask += (word_attn >= threshold).float()\n",
    "    \n",
    "    aggregated = word_attentions.mean(dim=0)\n",
    "    mask = (combined_mask > 0).float()\n",
    "    \n",
    "    return aggregated.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel 1: POS-weighted\n",
    "def aggregate_pos_weighted(word_attentions, words, k_percent=0.5):\n",
    "    \"\"\"Weight attention by part-of-speech importance\"\"\"\n",
    "    doc = nlp(\" \".join(words))\n",
    "    \n",
    "    weighted_attn = torch.zeros_like(word_attentions[0])\n",
    "    for i, (word, token) in enumerate(zip(words, doc)):\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            weight = 2.0\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            weight = 1.5\n",
    "        elif token.pos_ == \"ADJ\":\n",
    "            weight = 1.2\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        weighted_attn += word_attentions[i] * weight\n",
    "    \n",
    "    aggregated = weighted_attn / len(words)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(aggregated.flatten(), k).values.min()\n",
    "    mask = (aggregated >= threshold).float()\n",
    "    \n",
    "    return aggregated.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel 2: Temporal-decay\n",
    "def aggregate_temporal_decay(word_attentions, k_percent=0.5, decay=0.9):\n",
    "    \"\"\"Weight earlier words higher with exponential decay\"\"\"\n",
    "    weighted_attn = torch.zeros_like(word_attentions[0])\n",
    "    \n",
    "    for i, word_attn in enumerate(word_attentions):\n",
    "        weight = decay ** i\n",
    "        weighted_attn += word_attn * weight\n",
    "    \n",
    "    aggregated = weighted_attn / len(word_attentions)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(aggregated.flatten(), k).values.min()\n",
    "    mask = (aggregated >= threshold).float()\n",
    "    \n",
    "    return aggregated.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel 3: Variance-based\n",
    "def aggregate_variance_based(word_attentions, k_percent=0.5):\n",
    "    \"\"\"Select pixels with high variance across words\"\"\"\n",
    "    variance = word_attentions.var(dim=0)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(variance.flatten(), k).values.min()\n",
    "    mask = (variance >= threshold).float()\n",
    "    \n",
    "    return variance.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel 4: Entropy-based\n",
    "def aggregate_entropy_based(word_attentions, k_percent=0.5):\n",
    "    \"\"\"Select pixels with high entropy (uncertainty) across words\"\"\"\n",
    "    word_attentions_norm = torch.softmax(word_attentions.flatten(1), dim=1)\n",
    "    word_attentions_norm = word_attentions_norm.view_as(word_attentions)\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "    entropy = -(word_attentions_norm * torch.log(word_attentions_norm + epsilon)).sum(dim=0)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(entropy.flatten(), k).values.min()\n",
    "    mask = (entropy >= threshold).float()\n",
    "    \n",
    "    return entropy.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "print(\"All 6 aggregation methods loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def visualize_attention_overlay(img, pred_caption, agg_attn, mask, method_name, k_viz=0.3):\n",
    "    \"\"\"\n",
    "    Visualize attention with overlay\n",
    "    k_viz: percentage for visualization (30% for clarity)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Original\\n{pred_caption[:40]}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    img_array = np.array(img.resize((384, 384)))\n",
    "    attn_resized = np.array(Image.fromarray((agg_attn * 255).astype(np.uint8)).resize((384, 384)))\n",
    "    attn_resized = attn_resized / 255.0\n",
    "    \n",
    "    axes[1].imshow(img_array)\n",
    "    axes[1].imshow(attn_resized, cmap=\"hot\", alpha=0.5)\n",
    "    axes[1].set_title(f\"{method_name}\\nHeatmap Overlay\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    axes[2].imshow(agg_attn, cmap=\"hot\", interpolation=\"bilinear\")\n",
    "    axes[2].set_title(\"Attention Heatmap\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    # Recompute mask for visualization\n",
    "    agg_tensor = torch.from_numpy(agg_attn) if isinstance(agg_attn, np.ndarray) else agg_attn\n",
    "    k_viz_pixels = int(576 * k_viz)\n",
    "    threshold_viz = torch.topk(agg_tensor.flatten(), k_viz_pixels).values.min()\n",
    "    mask_viz = (agg_tensor >= threshold_viz).float().numpy()\n",
    "    \n",
    "    mask_resized = np.array(Image.fromarray((mask_viz * 255).astype(np.uint8)).resize((384, 384), Image.NEAREST))\n",
    "    mask_resized = mask_resized / 255.0\n",
    "    \n",
    "    axes[3].imshow(img_array)\n",
    "    axes[3].imshow(mask_resized, cmap=\"Reds\", alpha=0.4)\n",
    "    axes[3].set_title(f\"Top {int(k_viz*100)}% Patches\\nSelected for Attack\")\n",
    "    axes[3].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function with adjustable k_percent\n",
    "def visualize_attention_overlay(img, pred_caption, agg_attn, mask, method_name=\"Sentence-based\", k_viz=0.3):\n",
    "    \"\"\"\n",
    "    Visualize attention with overlay on original image\n",
    "    k_viz: percentage for visualization (default 30% for clearer view)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Original\\n{pred_caption[:40]}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    img_array = np.array(img.resize((384, 384)))\n",
    "    attn_resized = np.array(Image.fromarray((agg_attn * 255).astype(np.uint8)).resize((384, 384)))\n",
    "    attn_resized = attn_resized / 255.0\n",
    "    \n",
    "    axes[1].imshow(img_array)\n",
    "    axes[1].imshow(attn_resized, cmap=\"hot\", alpha=0.5)\n",
    "    axes[1].set_title(f\"{method_name}\\nHeatmap Overlay\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    axes[2].imshow(agg_attn, cmap=\"hot\", interpolation=\"bilinear\")\n",
    "    axes[2].set_title(\"Attention Heatmap\\n(Smoothed)\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    # Recompute mask with k_viz for visualization\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        mask_tensor = torch.from_numpy(mask)\n",
    "        agg_tensor = torch.from_numpy(agg_attn)\n",
    "    else:\n",
    "        mask_tensor = mask\n",
    "        agg_tensor = agg_attn\n",
    "    \n",
    "    k_viz_pixels = int(576 * k_viz)\n",
    "    threshold_viz = torch.topk(agg_tensor.flatten(), k_viz_pixels).values.min()\n",
    "    mask_viz = (agg_tensor >= threshold_viz).float().numpy()\n",
    "    \n",
    "    mask_resized = np.array(Image.fromarray((mask_viz * 255).astype(np.uint8)).resize((384, 384), Image.NEAREST))\n",
    "    mask_resized = mask_resized / 255.0\n",
    "    \n",
    "    axes[3].imshow(img_array)\n",
    "    axes[3].imshow(mask_resized, cmap=\"Reds\", alpha=0.4)\n",
    "    axes[3].set_title(f\"Top {int(k_viz*100)}% Patches\\nSelected for Attack\")\n",
    "    axes[3].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test extraction\n",
    "idx = 87\n",
    "img, gt_caps, pred_caption, word_attns, words = extract_sat_attention(idx)\n",
    "\n",
    "print(f\"\\nWords: {words}\")\n",
    "print(f\"Attention shape: {word_attns.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all 6 methods on image 87\n",
    "idx = 87\n",
    "img, gt_caps, pred_caption, word_attns, words = extract_sat_attention(idx)\n",
    "\n",
    "methods = {\n",
    "    \"Sentence-based\": lambda: aggregate_sentence_based(word_attns, k_percent=0.5),\n",
    "    \"Word-based\": lambda: aggregate_word_based(word_attns, k_percent=0.5),\n",
    "    \"POS-weighted\": lambda: aggregate_pos_weighted(word_attns, words, k_percent=0.5),\n",
    "    \"Temporal-decay\": lambda: aggregate_temporal_decay(word_attns, k_percent=0.5),\n",
    "    \"Variance-based\": lambda: aggregate_variance_based(word_attns, k_percent=0.5),\n",
    "    \"Entropy-based\": lambda: aggregate_entropy_based(word_attns, k_percent=0.5)\n",
    "}\n",
    "\n",
    "for name, method in methods.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    agg, mask = method()\n",
    "    visualize_attention_overlay(img, pred_caption, agg, mask, name, k_viz=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
