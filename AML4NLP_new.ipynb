{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Basic imports for SAT model and attack\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.ndimage import zoom\nimport json\nimport os\nimport ssl\n\n# Fix SSL certificate issue on macOS\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"Imports loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SAT Encoder - uses pretrained ResNet101\nclass Encoder(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n        \n        # Use pretrained ResNet-101\n        resnet = models.resnet101(pretrained=True)\n        \n        # Remove linear and pool layers\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n        # Resize input images to fixed size for uniform outputs\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        \n        # Disable gradient computation for encoder\n        self.fine_tune(False)\n    \n    def forward(self, images):\n        # Input: (batch_size, 3, 256, 256)\n        # Output: (batch_size, 2048, enc_image_size, enc_image_size)\n        out = self.resnet(images)\n        out = self.adaptive_pool(out)\n        out = out.permute(0, 2, 3, 1)  # (batch_size, enc_image_size, enc_image_size, 2048)\n        return out\n    \n    def fine_tune(self, fine_tune=True):\n        # Allow or prevent gradient computation\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        # Only fine-tune conv layers 2-4\n        for c in list(self.resnet.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune\n\nprint(\"Encoder defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Attention module - computes soft attention weights\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n        self.full_att = nn.Linear(attention_dim, 1)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n    \n    def forward(self, encoder_out, decoder_hidden):\n        # encoder_out: (batch_size, num_pixels, encoder_dim)\n        # decoder_hidden: (batch_size, decoder_dim)\n        \n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n        \n        return attention_weighted_encoding, alpha\n\nprint(\"Attention module defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SAT Decoder with attention\nclass DecoderWithAttention(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n        super(DecoderWithAttention, self).__init__()\n        \n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        \n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)\n        self.init_weights()\n    \n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n    \n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n    \n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        # We won't use this during caption generation, just for training\n        # For generation, we use caption_with_attention method\n        pass\n\nprint(\"Decoder defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load or initialize SAT model\n# Model hyperparameters (from sgrvinod's implementation)\nencoder_dim = 2048\nattention_dim = 512\nembed_dim = 512\ndecoder_dim = 512\nvocab_size = 10000  # Will be updated when we load actual vocab\n\n# Initialize encoder and decoder\nencoder = Encoder().to(device)\ndecoder = DecoderWithAttention(\n    attention_dim=attention_dim,\n    embed_dim=embed_dim,\n    decoder_dim=decoder_dim,\n    vocab_size=vocab_size,\n    encoder_dim=encoder_dim\n).to(device)\n\n# Set to evaluation mode\nencoder.eval()\ndecoder.eval()\n\nprint(f\"SAT Model initialized\")\nprint(f\"Encoder output: 14x14 = 196 image patches\")\nprint(f\"Each patch: {encoder_dim} dimensions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Image preprocessing for SAT\n# SAT uses 256x256 images with ImageNet normalization\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n])\n\ndef load_image(image_path):\n    \"\"\"Load and preprocess a single image\"\"\"\n    img = Image.open(image_path).convert('RGB')\n    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n    return img, img_tensor\n\n# For testing, create a simple dummy image\n# You can replace this with actual COCO images later\ndummy_img = Image.new('RGB', (256, 256), color='blue')\nimg_original, img_tensor = dummy_img, transform(dummy_img).unsqueeze(0)\n\nprint(f\"Image tensor shape: {img_tensor.shape}\")\nprint(\"Image preprocessing ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Caption generation with attention extraction\ndef caption_with_attention(encoder, decoder, image_tensor, word_map, beam_size=1, max_len=20):\n    \"\"\"\n    Generate caption for image and return attention weights for each word.\n    \n    Returns:\n        - caption: list of word indices\n        - alphas: list of attention maps (one per word)\n    \"\"\"\n    # Encode image\n    encoder_out = encoder(image_tensor.to(device))  # (1, enc_image_size, enc_image_size, encoder_dim)\n    enc_image_size = encoder_out.size(1)\n    encoder_dim = encoder_out.size(3)\n    \n    # Flatten encoding\n    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n    num_pixels = encoder_out.size(1)  # Should be 196 (14x14)\n    \n    # Initialize LSTM state\n    h, c = decoder.init_hidden_state(encoder_out)  # (1, decoder_dim)\n    \n    # Start with <start> token\n    start_token = 1  # Assuming <start> token index is 1\n    prev_word = torch.LongTensor([start_token]).to(device)\n    \n    # Store generated words and attention weights\n    caption = []\n    alphas_list = []\n    \n    # Generate caption word by word\n    for t in range(max_len):\n        embeddings = decoder.embedding(prev_word)  # (1, embed_dim)\n        \n        # Get attention-weighted encoding\n        attention_weighted_encoding, alpha = decoder.attention(encoder_out, h)\n        alphas_list.append(alpha.cpu().detach())  # Save attention weights\n        \n        # Gating scalar\n        gate = decoder.sigmoid(decoder.f_beta(h))  # (1, encoder_dim)\n        attention_weighted_encoding = gate * attention_weighted_encoding\n        \n        # LSTM step\n        h, c = decoder.decode_step(\n            torch.cat([embeddings, attention_weighted_encoding], dim=1),\n            (h, c)\n        )\n        \n        # Predict next word\n        scores = decoder.fc(h)  # (1, vocab_size)\n        predicted = scores.argmax(dim=1)  # (1,)\n        \n        caption.append(predicted.item())\n        prev_word = predicted\n        \n        # Stop if <end> token (index 2)\n        if predicted.item() == 2:\n            break\n    \n    return caption, alphas_list\n\nprint(\"Caption generation with attention extraction ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sentence-based aggregation (Paper's main method)\ndef aggregate_sentence_based(alphas_list, k_percent=0.5):\n    \"\"\"\n    Aggregate attention across all words by summing.\n    Then select top-k% patches.\n    \n    Args:\n        alphas_list: List of attention tensors, one per word (each is (1, num_pixels))\n        k_percent: Percentage of patches to select (default 0.5 = 50%)\n    \n    Returns:\n        aggregated_2d: Attention heatmap as 2D array (14, 14)\n        mask_2d: Binary mask of top-k patches (14, 14)\n    \"\"\"\n    # Stack all attention maps and sum across words\n    all_alphas = torch.stack([a.squeeze(0) for a in alphas_list])  # (num_words, num_pixels)\n    aggregated = all_alphas.sum(dim=0)  # (num_pixels,) - sum across all words\n    \n    # Reshape to 2D grid (14x14 for SAT)\n    grid_size = int(np.sqrt(aggregated.shape[0]))  # Should be 14\n    aggregated_2d = aggregated.reshape(grid_size, grid_size)\n    \n    # Select top-k patches\n    k = int(aggregated.shape[0] * k_percent)\n    threshold = torch.topk(aggregated, k).values.min()\n    mask = (aggregated >= threshold).float()\n    mask_2d = mask.reshape(grid_size, grid_size)\n    \n    return aggregated_2d.numpy(), mask_2d.numpy()\n\nprint(\"Sentence-based aggregation ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Word-based aggregation (Paper's baseline)\ndef aggregate_word_based(alphas_list, k_percent=0.5):\n    \"\"\"\n    Select top-k patches per word, then take union.\n    \n    Args:\n        alphas_list: List of attention tensors, one per word\n        k_percent: Percentage of patches to select per word\n    \n    Returns:\n        aggregated_2d: Attention heatmap (averaged across words)\n        mask_2d: Binary mask (union of top-k patches across all words)\n    \"\"\"\n    num_words = len(alphas_list)\n    num_pixels = alphas_list[0].shape[1]\n    \n    # For visualization: average attention across words\n    all_alphas = torch.stack([a.squeeze(0) for a in alphas_list])\n    aggregated = all_alphas.mean(dim=0)\n    \n    # For mask: select top-k per word, then union\n    k_per_word = int(num_pixels * k_percent / num_words)\n    selected_patches = torch.zeros(num_pixels)\n    \n    for alpha in alphas_list:\n        alpha_flat = alpha.squeeze(0)\n        top_k_indices = torch.topk(alpha_flat, k_per_word).indices\n        selected_patches[top_k_indices] = 1\n    \n    # Reshape to 2D\n    grid_size = int(np.sqrt(num_pixels))\n    aggregated_2d = aggregated.reshape(grid_size, grid_size)\n    mask_2d = selected_patches.reshape(grid_size, grid_size)\n    \n    return aggregated_2d.numpy(), mask_2d.numpy()\n\nprint(\"Word-based aggregation ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization function\ndef visualize_attention(img, caption_text, aggregated_2d, mask_2d, method_name=\"Sentence-based\"):\n    \"\"\"\n    Visualize attention heatmap and mask overlay on image.\n    \n    Args:\n        img: Original PIL image\n        caption_text: Generated caption as string\n        aggregated_2d: Attention heatmap (14, 14)\n        mask_2d: Binary mask (14, 14)\n        method_name: Name of aggregation method\n    \"\"\"\n    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n    \n    # Original image\n    axes[0].imshow(img)\n    axes[0].set_title(f\"Original Image\\n{caption_text[:40]}\")\n    axes[0].axis(\"off\")\n    \n    # Attention heatmap overlay\n    img_resized = img.resize((256, 256))\n    img_array = np.array(img_resized)\n    \n    # Resize attention to match image size\n    from scipy.ndimage import zoom\n    zoom_factor = 256 / aggregated_2d.shape[0]\n    attn_resized = zoom(aggregated_2d, zoom_factor, order=1)\n    \n    axes[1].imshow(img_array)\n    axes[1].imshow(attn_resized, cmap=\"hot\", alpha=0.5)\n    axes[1].set_title(f\"{method_name}\\nAttention Overlay\")\n    axes[1].axis(\"off\")\n    \n    # Attention heatmap only\n    axes[2].imshow(aggregated_2d, cmap=\"hot\", interpolation=\"bilinear\")\n    axes[2].set_title(\"Attention Heatmap\\n(14x14 grid)\")\n    axes[2].axis(\"off\")\n    \n    # Top-k patches mask\n    mask_resized = zoom(mask_2d, zoom_factor, order=0)  # Nearest neighbor for binary mask\n    axes[3].imshow(img_array)\n    axes[3].imshow(mask_resized, cmap=\"Reds\", alpha=0.4)\n    axes[3].set_title(f\"Top {int(mask_2d.sum())} Patches\\nSelected for Attack\")\n    axes[3].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"Visualization ready\")"
  },
  {
   "cell_type": "code",
   "source": "# Test the complete pipeline\n# NOTE: This will generate random captions since we don't have pretrained weights yet\n# Once we load proper SAT weights, this will work correctly\n\nprint(\"Testing SAT pipeline...\")\nprint(\"=\" * 50)\n\n# Create dummy word map for testing\nword_map = {str(i): i for i in range(vocab_size)}\nreverse_word_map = {v: k for k, v in word_map.items()}\n\n# Generate caption with attention\nwith torch.no_grad():\n    caption_indices, alphas_list = caption_with_attention(\n        encoder, decoder, img_tensor, word_map, max_len=10\n    )\n\n# Convert indices to text (will be random without pretrained weights)\ncaption_text = \" \".join([reverse_word_map.get(idx, \"<unk>\") for idx in caption_indices])\nprint(f\"Generated caption: {caption_text}\")\nprint(f\"Number of words: {len(caption_indices)}\")\nprint(f\"Number of attention maps: {len(alphas_list)}\")\nprint(f\"Attention shape per word: {alphas_list[0].shape}\")\nprint()\n\n# Test sentence-based aggregation\nprint(\"Testing sentence-based aggregation...\")\nagg_sent, mask_sent = aggregate_sentence_based(alphas_list, k_percent=0.5)\nprint(f\"Aggregated attention shape: {agg_sent.shape}\")\nprint(f\"Number of selected patches: {mask_sent.sum()}\")\nprint()\n\n# Test word-based aggregation\nprint(\"Testing word-based aggregation...\")\nagg_word, mask_word = aggregate_word_based(alphas_list, k_percent=0.5)\nprint(f\"Aggregated attention shape: {agg_word.shape}\")\nprint(f\"Number of selected patches: {mask_word.sum()}\")\nprint()\n\nprint(\"Pipeline test complete!\")\nprint(\"=\" * 50)\nprint(\"\\nNOTE: To get proper captions, you need to:\")\nprint(\"1. Download pretrained SAT checkpoint from sgrvinod's repo\")\nprint(\"2. Load the checkpoint weights\")\nprint(\"3. Load the vocabulary (word_map.json)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize both aggregation methods\nprint(\"Visualizing sentence-based aggregation:\")\nvisualize_attention(img_original, caption_text, agg_sent, mask_sent, \"Sentence-based\")\n\nprint(\"\\nVisualizing word-based aggregation:\")\nvisualize_attention(img_original, caption_text, agg_word, mask_word, \"Word-based\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}