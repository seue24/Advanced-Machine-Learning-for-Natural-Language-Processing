{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import kagglehub\n",
    "import spacy\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO dataset\n",
    "path = kagglehub.dataset_download(\"nagasai524/mini-coco2014-dataset-for-image-captioning\")\n",
    "\n",
    "with open(os.path.join(path, \"captions.json\"), \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    annotations = data[\"annotations\"] if isinstance(data, dict) else data\n",
    "\n",
    "captions = {}\n",
    "for item in annotations:\n",
    "    img_id = item[\"image_id\"]\n",
    "    if img_id not in captions:\n",
    "        captions[img_id] = []\n",
    "    captions[img_id].append(item[\"caption\"])\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    if any(f.endswith(\".jpg\") for f in files):\n",
    "        img_folder = root\n",
    "        break\n",
    "\n",
    "img_ids = list(captions.keys())\n",
    "print(f\"Ready: {len(captions)} images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image loading function\n",
    "def get_image(idx):\n",
    "    \"\"\"Load and resize image to 384x384\"\"\"\n",
    "    img_id = img_ids[idx]\n",
    "    img_path = os.path.join(img_folder, f\"COCO_train2014_{img_id:012d}.jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        img_path = os.path.join(img_folder, f\"{img_id}.jpg\")\n",
    "    return Image.open(img_path).convert(\"RGB\").resize((384, 384)), captions[img_id]\n",
    "\n",
    "# Show selected image\n",
    "selected = [87]\n",
    "fig, axes = plt.subplots(1, len(selected), figsize=(8, 8))\n",
    "if len(selected) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, ax in zip(selected, axes):\n",
    "    img, caps = get_image(idx)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Image {idx}: {caps[0][:50]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "for idx in selected:\n",
    "    img, caps = get_image(idx)\n",
    "    print(f\"\\nImage {idx} captions:\")\n",
    "    for cap in caps:\n",
    "        print(f\"  - {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.48145466, 0.4578275, 0.40821073], \n",
    "                        [0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "def to_tensor(idx):\n",
    "    img, _ = get_image(idx)\n",
    "    return normalize(img).unsqueeze(0).to(device)\n",
    "\n",
    "test = to_tensor(0)\n",
    "print(f\"Tensor ready for BLIP: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
    "model.eval()\n",
    "print(\"BLIP model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption baseline\n",
    "def generate_blip_caption(idx, show_image=True):\n",
    "    \"\"\"Run BLIP on one COCO image and print GT and predicted captions\"\"\"\n",
    "    img, gt_caps = get_image(idx)\n",
    "\n",
    "    if show_image:\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"GT: {gt_caps[0][:60]}\")\n",
    "        plt.show()\n",
    "\n",
    "    inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "\n",
    "    pred_caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Image index: {idx}\")\n",
    "    print(f\"Ground truth: {gt_caps[0]}\")\n",
    "    print(f\"BLIP caption: {pred_caption}\")\n",
    "\n",
    "    return img, gt_caps, pred_caption\n",
    "\n",
    "# Test\n",
    "for i in [87]:\n",
    "    generate_blip_caption(i, show_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention (Paper Algorithm 1, lines 3-7)\n",
    "def extract_blip_attention(idx):\n",
    "    \"\"\"Extract vision attention from BLIP model\"\"\"\n",
    "    img, gt_caps = get_image(idx)\n",
    "    inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    vision_attentions = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        if len(output) > 1 and output[1] is not None:\n",
    "            vision_attentions.append(output[1].detach())\n",
    "    \n",
    "    hooks = []\n",
    "    for layer in model.vision_model.encoder.layers:\n",
    "        hook = layer.self_attn.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "        pred_caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    print(f\"Caption: {pred_caption}\")\n",
    "    print(f\"Captured {len(vision_attentions)} layers of attention\")\n",
    "    \n",
    "    return img, gt_caps, pred_caption, vision_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation methods (2 baselines + 4 novel methods)\n",
    "\n",
    "# Load spaCy for POS tagging\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    import os\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Baseline 1: Sentence-based (Paper's main method)\n",
    "def aggregate_sentence_based(vision_attentions, k_percent=0.5):\n",
    "    \"\"\"Aggregate attention across all layers\"\"\"\n",
    "    all_attention = []\n",
    "    for layer_attn in vision_attentions:\n",
    "        attn = layer_attn.mean(dim=1)\n",
    "        cls_to_patches = attn[0, 0, 1:]\n",
    "        all_attention.append(cls_to_patches)\n",
    "    \n",
    "    aggregated = torch.stack(all_attention).mean(dim=0)\n",
    "    aggregated_2d = aggregated.reshape(24, 24)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(aggregated.flatten(), k).values.min()\n",
    "    mask = (aggregated_2d >= threshold).float()\n",
    "    \n",
    "    return aggregated_2d.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Baseline 2: Word-based (Paper's baseline)\n",
    "def aggregate_word_based(vision_attentions, k_percent=0.5):\n",
    "    \"\"\"Select top-k patches per layer, then union\"\"\"\n",
    "    k_per_layer = int(576 * k_percent / len(vision_attentions))\n",
    "    selected_patches = torch.zeros(576)\n",
    "    \n",
    "    for layer_attn in vision_attentions:\n",
    "        attn = layer_attn.mean(dim=1)\n",
    "        cls_to_patches = attn[0, 0, 1:]\n",
    "        top_k_indices = torch.topk(cls_to_patches, k_per_layer).indices\n",
    "        selected_patches[top_k_indices] = 1\n",
    "    \n",
    "    aggregated_2d = selected_patches.reshape(24, 24)\n",
    "    \n",
    "    all_attention = []\n",
    "    for layer_attn in vision_attentions:\n",
    "        attn = layer_attn.mean(dim=1)\n",
    "        all_attention.append(attn[0, 0, 1:])\n",
    "    aggregated_vis = torch.stack(all_attention).mean(dim=0).reshape(24, 24)\n",
    "    \n",
    "    return aggregated_vis.cpu().numpy(), aggregated_2d.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel Method 1: POS-weighted\n",
    "def aggregate_pos_weighted(vision_attentions, caption, k_percent=0.5):\n",
    "    \"\"\"Weight attention by POS tags (nouns/verbs weighted higher)\"\"\"\n",
    "    doc = nlp(caption)\n",
    "    \n",
    "    weights = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            weights.append(2.0)\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            weights.append(1.5)\n",
    "        else:\n",
    "            weights.append(1.0)\n",
    "    \n",
    "    all_attention = []\n",
    "    for i, layer_attn in enumerate(vision_attentions):\n",
    "        attn = layer_attn.mean(dim=1)\n",
    "        cls_to_patches = attn[0, 0, 1:]\n",
    "        weight = weights[i % len(weights)]\n",
    "        all_attention.append(cls_to_patches * weight)\n",
    "    \n",
    "    aggregated = torch.stack(all_attention).mean(dim=0)\n",
    "    aggregated_2d = aggregated.reshape(24, 24)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(aggregated.flatten(), k).values.min()\n",
    "    mask = (aggregated_2d >= threshold).float()\n",
    "    \n",
    "    return aggregated_2d.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel Method 2: Temporal-decay\n",
    "def aggregate_temporal_decay(vision_attentions, k_percent=0.5, decay=0.95):\n",
    "    \"\"\"Later layers weighted lower (exponential decay)\"\"\"\n",
    "    all_attention = []\n",
    "    \n",
    "    for i, layer_attn in enumerate(vision_attentions):\n",
    "        attn = layer_attn.mean(dim=1)\n",
    "        cls_to_patches = attn[0, 0, 1:]\n",
    "        weight = decay ** i\n",
    "        all_attention.append(cls_to_patches * weight)\n",
    "    \n",
    "    aggregated = torch.stack(all_attention).mean(dim=0)\n",
    "    aggregated_2d = aggregated.reshape(24, 24)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(aggregated.flatten(), k).values.min()\n",
    "    mask = (aggregated_2d >= threshold).float()\n",
    "    \n",
    "    return aggregated_2d.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel Method 3: Variance-based\n",
    "def aggregate_variance_based(vision_attentions, k_percent=0.5):\n",
    "    \"\"\"Select patches with high variance across layers\"\"\"\n",
    "    all_attention = []\n",
    "    for layer_attn in vision_attentions:\n",
    "        attn = layer_attn.mean(dim=1)\n",
    "        cls_to_patches = attn[0, 0, 1:]\n",
    "        all_attention.append(cls_to_patches)\n",
    "    \n",
    "    all_attention = torch.stack(all_attention)\n",
    "    variance = all_attention.var(dim=0)\n",
    "    variance_2d = variance.reshape(24, 24)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(variance.flatten(), k).values.min()\n",
    "    mask = (variance_2d >= threshold).float()\n",
    "    \n",
    "    return variance_2d.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "\n",
    "# Novel Method 4: Entropy-based\n",
    "def aggregate_entropy_based(vision_attentions, k_percent=0.5):\n",
    "    \"\"\"Select patches with high entropy (uncertainty)\"\"\"\n",
    "    all_attention = []\n",
    "    for layer_attn in vision_attentions:\n",
    "        attn = layer_attn.mean(dim=1)\n",
    "        cls_to_patches = attn[0, 0, 1:]\n",
    "        all_attention.append(cls_to_patches)\n",
    "    \n",
    "    all_attention = torch.stack(all_attention)\n",
    "    all_attention = torch.softmax(all_attention, dim=1)\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "    entropy = -(all_attention * torch.log(all_attention + epsilon)).sum(dim=0)\n",
    "    entropy_2d = entropy.reshape(24, 24)\n",
    "    \n",
    "    k = int(576 * k_percent)\n",
    "    threshold = torch.topk(entropy.flatten(), k).values.min()\n",
    "    mask = (entropy_2d >= threshold).float()\n",
    "    \n",
    "    return entropy_2d.cpu().numpy(), mask.cpu().numpy()\n",
    "\n",
    "print(\"All 6 aggregation methods loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def visualize_attention_overlay(img, pred_caption, agg_attn, mask, method_name=\"Sentence-based\"):\n",
    "    \"\"\"Visualize attention with overlay on original image\"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Original\\n{pred_caption[:40]}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    img_array = np.array(img.resize((384, 384)))\n",
    "    attn_resized = np.array(Image.fromarray((agg_attn * 255).astype(np.uint8)).resize((384, 384)))\n",
    "    attn_resized = attn_resized / 255.0\n",
    "    \n",
    "    axes[1].imshow(img_array)\n",
    "    axes[1].imshow(attn_resized, cmap=\"hot\", alpha=0.5)\n",
    "    axes[1].set_title(f\"{method_name}\\nHeatmap Overlay\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    axes[2].imshow(agg_attn, cmap=\"hot\", interpolation=\"bilinear\")\n",
    "    axes[2].set_title(\"Attention Heatmap\\n(Smoothed)\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    mask_resized = np.array(Image.fromarray((mask * 255).astype(np.uint8)).resize((384, 384), Image.NEAREST))\n",
    "    mask_resized = mask_resized / 255.0\n",
    "    \n",
    "    axes[3].imshow(img_array)\n",
    "    axes[3].imshow(mask_resized, cmap=\"Reds\", alpha=0.4)\n",
    "    axes[3].set_title(f\"Top 50% Patches\\nSelected for Attack\")\n",
    "    axes[3].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all 6 methods on selected image\n",
    "idx = 87\n",
    "img, gt_caps, pred_caption, vision_attn = extract_blip_attention(idx)\n",
    "\n",
    "methods = {\n",
    "    \"Sentence-based\": lambda: aggregate_sentence_based(vision_attn),\n",
    "    \"Word-based\": lambda: aggregate_word_based(vision_attn),\n",
    "    \"POS-weighted\": lambda: aggregate_pos_weighted(vision_attn, pred_caption),\n",
    "    \"Temporal-decay\": lambda: aggregate_temporal_decay(vision_attn),\n",
    "    \"Variance-based\": lambda: aggregate_variance_based(vision_attn),\n",
    "    \"Entropy-based\": lambda: aggregate_entropy_based(vision_attn)\n",
    "}\n",
    "\n",
    "for name, method in methods.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    agg, mask = method()\n",
    "    visualize_attention_overlay(img, pred_caption, agg, mask, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
