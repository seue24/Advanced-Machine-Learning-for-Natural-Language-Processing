{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Image Captioning Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries, Set Seeds, and Select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Third party packages\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, ViTImageProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COCO Mini Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "data_path = kagglehub.dataset_download(\n",
    "    \"nagasai524/mini-coco2014-dataset-for-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions(dataset_path):\n",
    "    \"\"\"Load image_id -> list of captions mapping from captions.json.\"\"\"\n",
    "    captions_path = os.path.join(dataset_path, \"captions.json\")\n",
    "    with open(captions_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    annotations = data[\"annotations\"] if isinstance(data, dict) else data\n",
    "\n",
    "    captions = {}\n",
    "    for ann in annotations:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        captions.setdefault(img_id, []).append(ann[\"caption\"])\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_folder(dataset_path, ext=\".jpg\"):\n",
    "    \"\"\"Find first folder under dataset_path that contains image files.\"\"\"\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        if any(f.lower().endswith(ext) for f in files):\n",
    "            return root\n",
    "    raise RuntimeError(f\"No image folder with *{ext} found under {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = load_captions(data_path)\n",
    "img_folder = find_image_folder(data_path)\n",
    "img_ids = sorted(captions.keys())\n",
    "\n",
    "print(f\"Dataset loaded, {len(img_ids)} images with captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Loading Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(idx):\n",
    "    \"\"\"Load image and its captions by index in img_ids.\"\"\"\n",
    "    img_id = img_ids[idx]\n",
    "\n",
    "    # Try common filename patterns\n",
    "    candidates = [\n",
    "        os.path.join(img_folder, f\"COCO_train2014_{img_id:012d}.jpg\"),\n",
    "        os.path.join(img_folder, f\"{img_id}.jpg\"),\n",
    "    ]\n",
    "\n",
    "    for img_path in candidates:\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            return img, captions[img_id]\n",
    "\n",
    "    raise FileNotFoundError(f\"No image file found for image_id {img_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspection of a Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an image to show\n",
    "idx = 6000\n",
    "\n",
    "img, caps = get_image(idx)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image index: {idx}, image_id: {img_ids[idx]}\")\n",
    "print(\"Ground truth captions:\")\n",
    "for cap in caps:\n",
    "    print(f\"  - {cap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ViT-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"eager\").to(device)\n",
    "\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "print(f\"Decoder attention implementation: {model.config.decoder._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption_and_attention(img, max_length=16):\n",
    "    \"\"\"\n",
    "    Generate a caption for a PIL image and return:\n",
    "    - caption (str)\n",
    "    - cross_attentions (list of tensors)\n",
    "    - encoder_hidden_states (tensor)\n",
    "    \"\"\"\n",
    "    # Image -> pixel_values\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Encode image\n",
    "    encoder_outputs = model.encoder(pixel_values=pixel_values)\n",
    "    encoder_hidden_states = encoder_outputs.last_hidden_state  # [1, 197, hidden_dim]\n",
    "\n",
    "    # Generate caption ids\n",
    "    generated_ids = model.generate(\n",
    "        pixel_values,\n",
    "        max_length=max_length,\n",
    "        num_beams=1)\n",
    "\n",
    "    # Run decoder once over full sequence to get cross-attentions\n",
    "    decoder_outputs = model.decoder(\n",
    "        input_ids=generated_ids,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        output_attentions=True,\n",
    "        return_dict=True,\n",
    "        use_cache=False)\n",
    "\n",
    "    caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return caption, decoder_outputs.cross_attentions, encoder_hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cross_attention(cross_attentions, img_size, patch_size=14):\n",
    "    \"\"\"\n",
    "    Aggregate cross-attention over heads and tokens into a 2D heatmap\n",
    "    in image resolution (sentence-based aggregation).\n",
    "    \n",
    "    cross_attentions: list of tensors, each [1, num_heads, tgt_len, src_len]\n",
    "                      src_len = 1 (CLS) + patch_size^2\n",
    "    img_size: (width, height) of the original image\n",
    "    \"\"\"\n",
    "    # Take last decoder layer (most informative)\n",
    "    last_layer = cross_attentions[-1]          # [1, num_heads, tgt_len, src_len]\n",
    "    last_layer = last_layer.squeeze(0)         # [num_heads, tgt_len, src_len]\n",
    "\n",
    "    # Average over heads and decoder tokens\n",
    "    avg_over_heads = last_layer.mean(dim=0)    # [tgt_len, src_len]\n",
    "    avg_over_tokens = avg_over_heads.mean(dim=0)  # [src_len]\n",
    "\n",
    "    # Drop CLS token and keep only patch tokens\n",
    "    patch_attention = avg_over_tokens[1:]      # [patch_size^2]\n",
    "\n",
    "    # Reshape to patch grid\n",
    "    attention_map = patch_attention.reshape(patch_size, patch_size).cpu().numpy()\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    attention_map -= attention_map.min()\n",
    "    denom = attention_map.max() if attention_map.max() > 0 else 1e-8\n",
    "    attention_map /= denom\n",
    "\n",
    "    # Upsample to original image size\n",
    "    w, h = img_size\n",
    "    attn_img = Image.fromarray((attention_map * 255).astype(np.uint8))\n",
    "    attn_resized = attn_img.resize((w, h), resample=Image.BICUBIC)\n",
    "\n",
    "    return np.array(attn_resized) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_attention(img, caption, attention_heatmap):\n",
    "    \"\"\"Plot original image, attention heatmap, and overlay.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # 1) Original image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Original image\\nCaption: '{caption}'\", fontsize=10)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # 2) Heatmap only\n",
    "    axes[1].imshow(attention_heatmap, cmap=\"hot\")\n",
    "    axes[1].set_title(\"Cross-attention heatmap\\n(sentence-based)\", fontsize=10)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # 3) Overlay\n",
    "    axes[2].imshow(img)\n",
    "    axes[2].imshow(attention_heatmap, cmap=\"hot\", alpha=0.5)\n",
    "    axes[2].set_title(\"Attention overlay\\n(red = high attention)\", fontsize=10)\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Pick an Image, Get Caption and Attention and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt_caps = get_image(idx)\n",
    "\n",
    "caption, cross_attentions, _ = generate_caption_and_attention(img)\n",
    "attention_heatmap = aggregate_cross_attention(cross_attentions, img.size)\n",
    "\n",
    "show_image_and_attention(img, caption, attention_heatmap)\n",
    "\n",
    "print(f\"Image index: {idx}, image_id: {img_ids[idx]}\")\n",
    "print(\"\\nGround-truth captions:\")\n",
    "for cap in gt_caps:\n",
    "    print(f\"  - {cap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Candidate Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_candidate_pixels(attention_heatmap, k):\n",
    "    \"\"\"\n",
    "    Select top-k pixels with highest attention as candidate region.\n",
    "    Returns list of (y, x) indices.\n",
    "    \"\"\"\n",
    "    flat = attention_heatmap.flatten()\n",
    "    k = min(k, flat.size)\n",
    "    idxs = np.argpartition(flat, -k)[-k:]\n",
    "    ys, xs = np.unravel_index(idxs, attention_heatmap.shape)\n",
    "    return list(zip(ys, xs))\n",
    "\n",
    "\n",
    "def show_candidate_region(img, attention_heatmap, candidate_pixels):\n",
    "    \"\"\"\n",
    "    Visualize original image and candidate region (pixels we may perturb).\n",
    "    \"\"\"\n",
    "    mask = np.zeros(attention_heatmap.shape, dtype=bool)\n",
    "    for y, x in candidate_pixels:\n",
    "        mask[y, x] = True\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Original image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Original image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Candidate overlay\n",
    "    axes[1].imshow(img)\n",
    "    axes[1].imshow(mask, cmap=\"Reds\", alpha=0.6)\n",
    "    axes[1].set_title(\"Candidate region (pixels we may perturb)\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_IDX = idx\n",
    "\n",
    "# Image and Ground Truth Captions\n",
    "base_img, gt_caps = get_image(ATTACK_IDX)\n",
    "\n",
    "# 2) Caption + Cross-Attention vom Modell holen\n",
    "base_caption, cross_attentions, _ = generate_caption_and_attention(base_img)\n",
    "\n",
    "# 3) Attention-Heatmap aggregieren\n",
    "attention_heatmap = aggregate_cross_attention(cross_attentions, base_img.size)\n",
    "\n",
    "# 4) Top-k Pixel als Candidate Region wählen\n",
    "candidate_pixels = get_topk_candidate_pixels(attention_heatmap, k=1000)\n",
    "\n",
    "print(f\"Attack index: {ATTACK_IDX}\")\n",
    "print(f\"Base caption: '{base_caption}'\")\n",
    "print(f\"Number of candidate pixels: {len(candidate_pixels)}\")\n",
    "\n",
    "# 5) Candidate Region visualisieren\n",
    "show_candidate_region(base_img, attention_heatmap, candidate_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(img):\n",
    "    \"\"\"Convert PIL image to uint8 numpy array of shape (H, W, 3).\"\"\"\n",
    "    return np.array(img).astype(np.uint8)\n",
    "\n",
    "\n",
    "def array_to_image(arr):\n",
    "    \"\"\"Convert numpy array (H, W, 3) back to PIL image.\"\"\"\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "\n",
    "def apply_perturbation(base_array, delta_vec, candidate_pixels):\n",
    "    \"\"\"\n",
    "    Apply a perturbation vector to the candidate pixels of an image.\n",
    "    \n",
    "    base_array: numpy array (H, W, 3), uint8\n",
    "    delta_vec: 1D numpy array of length 3 * len(candidate_pixels)\n",
    "               [dR1, dG1, dB1, dR2, dG2, dB2, ...]\n",
    "    candidate_pixels: list of (y, x) positions\n",
    "    \"\"\"\n",
    "    H, W, C = base_array.shape\n",
    "    assert C == 3, \"Expected RGB image\"\n",
    "    num_pixels = len(candidate_pixels)\n",
    "    assert delta_vec.shape[0] == 3 * num_pixels, \"delta_vec length mismatch\"\n",
    "\n",
    "    # work on a float copy so we can add positive/negative deltas\n",
    "    perturbed = base_array.astype(np.float32).copy()\n",
    "    deltas = delta_vec.reshape(num_pixels, 3)\n",
    "\n",
    "    for (y, x), d in zip(candidate_pixels, deltas):\n",
    "        # add deltas to this pixel (R,G,B)\n",
    "        perturbed[y, x, :] += d\n",
    "\n",
    "    # clip back to valid image range\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return array_to_image(perturbed)\n",
    "\n",
    "\n",
    "# base image as numpy array (used for all perturbations)\n",
    "base_array = image_to_array(base_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption(img, max_length=16):\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values, max_length=max_length, num_beams=1)\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Prepare token IDs of the base caption (this is the fixed target for the loss)\n",
    "base_ids = tokenizer(\n",
    "    base_caption,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    ").input_ids.to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def caption_loss_for_image(img):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss of the fixed base caption given the image.\n",
    "    Higher loss = caption less compatible with the image.\n",
    "    \"\"\"\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    outputs = model(pixel_values=pixel_values, labels=base_ids)\n",
    "    return outputs.loss.item()\n",
    "\n",
    "\n",
    "# Loss for the original (unperturbed) image\n",
    "base_loss = caption_loss_for_image(base_img)\n",
    "print(f\"Base loss for original image: {base_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better fitness with actual caption comparison\n",
    "@torch.no_grad()\n",
    "def fitness_caption_change(x):\n",
    "    \"\"\"\n",
    "    Fitness based on actual caption change\n",
    "    \"\"\"\n",
    "    perturbed_img = apply_perturbation(base_array, x, candidate_pixels)\n",
    "    adv_caption = generate_caption(perturbed_img)\n",
    "    \n",
    "    # Jaccard distance between word sets\n",
    "    base_words = set(base_caption.lower().split())\n",
    "    adv_words = set(adv_caption.lower().split())\n",
    "    \n",
    "    if not base_words or not adv_words:\n",
    "        overlap = 0.0\n",
    "    else:\n",
    "        overlap = len(base_words & adv_words) / len(base_words | adv_words)\n",
    "    \n",
    "    # Perturbation size penalty\n",
    "    norm_penalty = LAMBDA_REG * (np.linalg.norm(x) / (255.0 * np.sqrt(x.size)))\n",
    "    \n",
    "    # Lower overlap = better attack\n",
    "    # Return negative (DE maximizes, we want to minimize overlap)\n",
    "    fitness_score = -(overlap + norm_penalty)\n",
    "    \n",
    "    return fitness_score\n",
    "\n",
    "# Run with better hyperparameters\n",
    "LAMBDA_REG = 0.0  # no regularization for first test\n",
    "\n",
    "best_x, best_f = differential_evolution(\n",
    "    fitness_fn=fitness_caption_change,\n",
    "    dim=dim,\n",
    "    pop_size=20,      # more individuals\n",
    "    generations=10,   # more iterations\n",
    "    F=0.8,           # stronger mutation\n",
    "    CR=0.9,          # higher crossover\n",
    "    eps=50.0,        # allow stronger perturbations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_evolution(\n",
    "    fitness_fn,\n",
    "    dim,\n",
    "    pop_size=8,\n",
    "    generations=5,\n",
    "    F=0.5,\n",
    "    CR=0.7,\n",
    "    eps=20.0,\n",
    "    random_seed=42):\n",
    "    \"\"\"\n",
    "    Very simple Differential Evolution (DE/rand/1/bin).\n",
    "    \n",
    "    fitness_fn: function x -> scalar (higher = better)\n",
    "    dim: dimensionality of x (here: 3 * len(candidate_pixels))\n",
    "    pop_size: number of individuals in the population\n",
    "    generations: number of DE generations\n",
    "    F: mutation factor\n",
    "    CR: crossover rate\n",
    "    eps: initial range for parameters: x ~ U[-eps, +eps]\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "\n",
    "    # 1) Initialize population with random vectors\n",
    "    pop = rng.uniform(low=-eps, high=eps, size=(pop_size, dim)).astype(np.float32)\n",
    "    fitness_vals = np.array([fitness_fn(ind) for ind in pop], dtype=np.float32)\n",
    "\n",
    "    best_idx = int(np.argmax(fitness_vals))\n",
    "    best_x = pop[best_idx].copy()\n",
    "    best_f = float(fitness_vals[best_idx])\n",
    "\n",
    "    print(f\"Initial best fitness: {best_f:.4f}\")\n",
    "\n",
    "    # 2) Iterate over generations\n",
    "    for gen in range(generations):\n",
    "        print(f\"\\n--- Generation {gen + 1}/{generations} ---\")\n",
    "\n",
    "        for i in range(pop_size):\n",
    "            # Choose indices for mutation (a, b, c all different and all != i)\n",
    "            idxs = list(range(pop_size))\n",
    "            idxs.remove(i)\n",
    "            a, b, c = rng.choice(idxs, size=3, replace=False)\n",
    "\n",
    "            x_a, x_b, x_c = pop[a], pop[b], pop[c]\n",
    "\n",
    "            # Mutation: v = x_a + F * (x_b - x_c)\n",
    "            v = x_a + F * (x_b - x_c)\n",
    "\n",
    "            # Crossover: binomial\n",
    "            cross_mask = rng.random(dim) < CR\n",
    "            # Ensure at least one component comes from v\n",
    "            j_rand = rng.integers(0, dim)\n",
    "            cross_mask[j_rand] = True\n",
    "\n",
    "            u = np.where(cross_mask, v, pop[i])\n",
    "\n",
    "            # Evaluate trial vector\n",
    "            f_u = fitness_fn(u)\n",
    "\n",
    "            # Selection: keep trial if it is better\n",
    "            if f_u > fitness_vals[i]:\n",
    "                pop[i] = u\n",
    "                fitness_vals[i] = f_u\n",
    "\n",
    "                # Update global best if necessary\n",
    "                if f_u > best_f:\n",
    "                    best_f = float(f_u)\n",
    "                    best_x = u.copy()\n",
    "\n",
    "        print(f\"Best fitness after generation {gen + 1}: {best_f:.4f}\")\n",
    "\n",
    "    return best_x, best_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the search space\n",
    "dim = 3 * len(candidate_pixels)\n",
    "print(f\"Dimensionality of x: {dim}\")\n",
    "\n",
    "best_x, best_f = differential_evolution(\n",
    "    fitness_fn=fitness,\n",
    "    dim=dim,\n",
    "    pop_size=6,      # keep small because each call runs the model\n",
    "    generations=3,   # small number of iterations for testing\n",
    "    F=0.6,\n",
    "    CR=0.7,\n",
    "    eps=20.0,       # allow strong per-channel perturbations\n",
    ")\n",
    "\n",
    "print(\"\\n=== Differential Evolution finished ===\")\n",
    "print(f\"Best fitness found (loss increase): {best_f:.4f}\")\n",
    "\n",
    "# Create best adversarial image from the found perturbation\n",
    "best_img = apply_perturbation(base_array, best_x, candidate_pixels)\n",
    "best_caption = generate_caption(best_img)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(base_img)\n",
    "plt.title(f\"Original\\n'{base_caption}'\", fontsize=9)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(best_img)\n",
    "plt.title(f\"Adversarial\\n'{best_caption}'\", fontsize=9)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOriginal caption : '{base_caption}'\")\n",
    "print(f\"Adversarial cap. : '{best_caption}'\")\n",
    "print(f\"Base loss        : {base_loss:.4f}\")\n",
    "print(f\"Best loss (base+f): {base_loss + best_f:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what magnitudes the DE is actually using\n",
    "print(f\"Best perturbation stats:\")\n",
    "print(f\"  Min delta: {best_x.min():.2f}\")\n",
    "print(f\"  Max delta: {best_x.max():.2f}\")\n",
    "print(f\"  Mean abs:  {np.abs(best_x).mean():.2f}\")\n",
    "print(f\"  Std:       {best_x.std():.2f}\")\n",
    "\n",
    "# Check L-infinity norm (max single pixel change)\n",
    "deltas_reshaped = best_x.reshape(-1, 3)\n",
    "per_pixel_max = np.abs(deltas_reshaped).max(axis=1)\n",
    "print(f\"\\n  L-inf norm (max pixel change): {per_pixel_max.max():.2f}\")\n",
    "print(f\"  Number of pixels with change > 10: {(per_pixel_max > 10).sum()}\")\n",
    "print(f\"  Number of pixels with change > 30: {(per_pixel_max > 30).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk if needed\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def compute_bleu(reference_caption, candidate_caption):\n",
    "    \"\"\"\n",
    "    Compute BLEU score between reference and candidate.\n",
    "    Lower BLEU = more different = better attack\n",
    "    \"\"\"\n",
    "    ref_tokens = reference_caption.lower().split()\n",
    "    cand_tokens = candidate_caption.lower().split()\n",
    "    \n",
    "    # BLEU needs list of references (can be multiple)\n",
    "    references = [ref_tokens]\n",
    "    \n",
    "    # Smoothing function to avoid zero scores\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # Compute BLEU (0 to 1, higher = more similar)\n",
    "    bleu = sentence_bleu(references, cand_tokens, smoothing_function=smoothing)\n",
    "    \n",
    "    return bleu\n",
    "\n",
    "# Test\n",
    "test_bleu = compute_bleu(base_caption, base_caption)\n",
    "print(f\"BLEU(same caption): {test_bleu:.4f}\")\n",
    "\n",
    "test_bleu2 = compute_bleu(base_caption, \"a dog running in water\")\n",
    "print(f\"BLEU(different caption): {test_bleu2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def fitness_bleu(x):\n",
    "    \"\"\"\n",
    "    Fitness based on BLEU score (as in the paper).\n",
    "    Lower BLEU = better attack.\n",
    "    We return negative BLEU because DE maximizes.\n",
    "    \"\"\"\n",
    "    # Apply perturbation\n",
    "    perturbed_img = apply_perturbation(base_array, x, candidate_pixels)\n",
    "    \n",
    "    # Generate caption\n",
    "    adv_caption = generate_caption(perturbed_img)\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu = compute_bleu(base_caption, adv_caption)\n",
    "    \n",
    "    # L2 penalty for perturbation size\n",
    "    norm_penalty = LAMBDA_REG * (np.linalg.norm(x) / (255.0 * np.sqrt(x.size)))\n",
    "    \n",
    "    # Return negative (DE maximizes, we want to minimize BLEU + penalty)\n",
    "    return -(bleu + norm_penalty)\n",
    "\n",
    "# Quick test\n",
    "print(\"Testing BLEU-based fitness:\")\n",
    "x_zero = np.zeros(dim, dtype=np.float32)\n",
    "f_zero = fitness_bleu(x_zero)\n",
    "print(f\"  Fitness(zero perturbation): {f_zero:.4f}\")\n",
    "\n",
    "x_random = np.random.uniform(-50, 50, size=dim).astype(np.float32)\n",
    "f_random = fitness_bleu(x_random)\n",
    "print(f\"  Fitness(random perturbation): {f_random:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# AICAttack-Style Pipeline v3 (ViT-GPT2)\n",
    "# - größere Candidate Region\n",
    "# - Fitness = Caption-Loss + Bonus bei Caption-Change\n",
    "# - eigene DE-Implementierung (Maximierung)\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Hilfsfunktionen für Captioning + Attention\n",
    "# --------------------------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption(\n",
    "    img,\n",
    "    max_length: int = 20,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper um model.generate für deterministische oder stochastische Captioning.\n",
    "    \"\"\"\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_length=max_length,\n",
    "        num_beams=1,\n",
    "        do_sample=do_sample,\n",
    "    )\n",
    "    if do_sample:\n",
    "        gen_kwargs.update(dict(temperature=temperature, top_p=top_p))\n",
    "\n",
    "    output_ids = model.generate(pixel_values=pixel_values, **gen_kwargs)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption_and_cross_attention(img, max_length: int = 20):\n",
    "    \"\"\"\n",
    "    1) Deterministische Caption\n",
    "    2) Cross-Attention Maps zu genau dieser Caption\n",
    "    \"\"\"\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Schritt 1: deterministische Caption generieren\n",
    "    gen_ids = model.generate(\n",
    "        pixel_values=pixel_values,\n",
    "        max_length=max_length,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    base_caption = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Schritt 2: Encoder + Decoder mit output_attentions=True\n",
    "    encoder_outputs = model.encoder(pixel_values=pixel_values)\n",
    "    encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "\n",
    "    decoder_outputs = model.decoder(\n",
    "        input_ids=gen_ids,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        output_attentions=True,\n",
    "        return_dict=True,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    cross_attentions = decoder_outputs.cross_attentions\n",
    "\n",
    "    return base_caption, cross_attentions\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Bild auswählen, Base-Caption + Attention holen\n",
    "# --------------------------------------------------\n",
    "\n",
    "ATTACK_IDX = idx  # oder fix eine Zahl wählen\n",
    "base_img, gt_caps = get_image(ATTACK_IDX)\n",
    "\n",
    "base_caption, cross_attentions = generate_caption_and_cross_attention(\n",
    "    base_img,\n",
    "    max_length=20,\n",
    ")\n",
    "\n",
    "attention_heatmap = aggregate_cross_attention(cross_attentions, base_img.size)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Candidate Region: Top-k Attention-Pixel (größer)\n",
    "# --------------------------------------------------\n",
    "\n",
    "TOPK_PIXELS = 30000  # vorher 600 → jetzt deutlich mehr Fläche\n",
    "\n",
    "candidate_pixels = get_topk_candidate_pixels(attention_heatmap, k=TOPK_PIXELS)\n",
    "\n",
    "print(f\"Attack index: {ATTACK_IDX}\")\n",
    "print(f\"Base caption: '{base_caption}'\")\n",
    "print(f\"Number of candidate pixels: {len(candidate_pixels)}\")\n",
    "\n",
    "show_candidate_region(base_img, attention_heatmap, candidate_pixels)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) Vorbereitung für DE\n",
    "# --------------------------------------------------\n",
    "\n",
    "base_array = image_to_array(base_img)  # (H, W, 3), uint8\n",
    "num_pixels = len(candidate_pixels)\n",
    "dim = 3 * num_pixels  # (dR, dG, dB) pro Pixel\n",
    "\n",
    "# Base-Caption als Labels für Loss\n",
    "base_caption_ids = tokenizer(\n",
    "    base_caption,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    ").input_ids.to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def caption_loss_for_image(img) -> float:\n",
    "    \"\"\"\n",
    "    Cross-Entropy-Loss der fixen Base-Caption unter dem Bild.\n",
    "    \"\"\"\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    outputs = model(pixel_values=pixel_values, labels=base_caption_ids)\n",
    "    return float(outputs.loss)\n",
    "\n",
    "\n",
    "BASE_LOSS = caption_loss_for_image(base_img)\n",
    "print(f\"\\nBase caption loss: {BASE_LOSS:.4f}\")\n",
    "\n",
    "# Attack-Strength: etwas kleiner als vorher, dafür mehr Pixel\n",
    "EPS_BOUND = 150.0\n",
    "\n",
    "\n",
    "def clamp_and_fix_dim(delta_vec: np.ndarray, dim: int, bound: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Passt Länge auf 'dim' an und begrenzt Werte auf [-bound, +bound].\n",
    "    \"\"\"\n",
    "    if delta_vec.shape[0] != dim:\n",
    "        if delta_vec.shape[0] > dim:\n",
    "            delta_vec = delta_vec[:dim]\n",
    "        else:\n",
    "            delta_vec = np.pad(delta_vec, (0, dim - delta_vec.shape[0]), mode=\"constant\")\n",
    "    return np.clip(delta_vec, -bound, bound)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def fitness_loss_plus_caption_change(delta_vec: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Fitness für DE:\n",
    "    - wendet Delta auf Candidate Region an\n",
    "    - berechnet Caption-Loss\n",
    "    - generiert stochastische Caption (schneller sensibel für Logit-Shifts)\n",
    "    - gibt Loss + Bonus, falls Caption != Base zurück\n",
    "    \"\"\"\n",
    "    delta = clamp_and_fix_dim(delta_vec, dim=dim, bound=EPS_BOUND)\n",
    "    adv_img = apply_perturbation(base_array, delta, candidate_pixels)\n",
    "\n",
    "    # 1) Loss der Base-Caption\n",
    "    loss = caption_loss_for_image(adv_img)\n",
    "\n",
    "    # 2) Stochastische Caption für Change-Bonus\n",
    "    adv_cap_sample = generate_caption(\n",
    "        adv_img,\n",
    "        do_sample=True,\n",
    "        temperature=1.5,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    changed = 0.0 if adv_cap_sample.strip() == base_caption.strip() else 1.0\n",
    "\n",
    "    # Bonus-Weight: Base-Loss liegt ~9, also +5 reicht,\n",
    "    # um Zustände mit Caption-Change klar zu bevorzugen.\n",
    "    ALPHA = 5.0\n",
    "    fitness = loss + ALPHA * changed\n",
    "    return float(fitness)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5) Eigene Differential Evolution (Maximierung)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def differential_evolution_v2(\n",
    "    fitness_fn,\n",
    "    dim: int,\n",
    "    pop_size: int = 60,\n",
    "    generations: int = 8,\n",
    "    F: float = 0.8,\n",
    "    CR: float = 0.9,\n",
    "    eps: float = 100,\n",
    "    random_seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Klassisches Differential Evolution (DE/rand/1/bin), wir MAXIMIEREN fitness_fn.\n",
    "    Alle Parameter leben in [-eps, +eps].\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "\n",
    "    # Initialpopulation\n",
    "    pop = rng.uniform(-eps, eps, size=(pop_size, dim))\n",
    "    fitness = np.array([fitness_fn(ind) for ind in pop])\n",
    "    best_idx = np.argmax(fitness)\n",
    "    best = pop[best_idx].copy()\n",
    "    best_fit = fitness[best_idx]\n",
    "\n",
    "    print(f\"\\nInitial best fitness: {best_fit:.4f}\")\n",
    "\n",
    "    for g in range(generations):\n",
    "        for i in range(pop_size):\n",
    "            # drei verschiedene Individuen r1, r2, r3\n",
    "            idxs = np.arange(pop_size)\n",
    "            idxs = idxs[idxs != i]\n",
    "            r1, r2, r3 = rng.choice(idxs, size=3, replace=False)\n",
    "            x1, x2, x3 = pop[r1], pop[r2], pop[r3]\n",
    "\n",
    "            # Mutation\n",
    "            mutant = x1 + F * (x2 - x3)\n",
    "\n",
    "            # Crossover\n",
    "            cross_points = rng.random(dim) < CR\n",
    "            if not np.any(cross_points):\n",
    "                cross_points[rng.integers(0, dim)] = True\n",
    "            trial = np.where(cross_points, mutant, pop[i])\n",
    "\n",
    "            # Begrenzen\n",
    "            trial = np.clip(trial, -eps, eps)\n",
    "\n",
    "            # Fitness auswerten\n",
    "            f_trial = fitness_fn(trial)\n",
    "\n",
    "            # Selektion (MAXIMIEREN)\n",
    "            if f_trial > fitness[i]:\n",
    "                pop[i] = trial\n",
    "                fitness[i] = f_trial\n",
    "                if f_trial > best_fit:\n",
    "                    best_fit = f_trial\n",
    "                    best = trial\n",
    "\n",
    "        print(f\"--- Generation {g+1}/{generations} ---\")\n",
    "        print(f\"Best fitness after generation {g+1}: {best_fit:.4f}\")\n",
    "\n",
    "    return best, best_fit\n",
    "\n",
    "\n",
    "# Hyperparameter für DE\n",
    "POP_SIZE = 60\n",
    "GENERATIONS = 2   # etwas höher, damit wir eine Chance auf Caption-Change haben\n",
    "DE_F = 0.8\n",
    "DE_CR = 0.9\n",
    "\n",
    "best_x, best_f = differential_evolution_v2(\n",
    "    fitness_fn=fitness_loss_plus_caption_change,\n",
    "    dim=dim,\n",
    "    pop_size=POP_SIZE,\n",
    "    generations=GENERATIONS,\n",
    "    F=DE_F,\n",
    "    CR=DE_CR,\n",
    "    eps=EPS_BOUND,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Differential Evolution finished ===\")\n",
    "print(f\"Best fitness found: {best_f:.4f}  (base loss: {BASE_LOSS:.4f})\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6) Bestes adversariales Bild + Captions erzeugen\n",
    "# --------------------------------------------------\n",
    "\n",
    "best_delta = clamp_and_fix_dim(best_x, dim=dim, bound=EPS_BOUND)\n",
    "adv_img = apply_perturbation(base_array, best_delta, candidate_pixels)\n",
    "\n",
    "adv_caption_det = generate_caption(adv_img, do_sample=False)\n",
    "adv_caption_samp = generate_caption(\n",
    "    adv_img,\n",
    "    do_sample=True,\n",
    "    temperature=1.5,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "print(\"\\nBase caption:         \", base_caption)\n",
    "print(\"Adv caption (det):    \", adv_caption_det)\n",
    "print(\"Adv caption (sample): \", adv_caption_samp)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7) Visualisierung\n",
    "# --------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(base_img)\n",
    "plt.title(f\"Original\\n'{base_caption}'\", fontsize=9)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(adv_img)\n",
    "plt.title(f\"Adversarial\\n'{adv_caption_det}'\", fontsize=9)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
