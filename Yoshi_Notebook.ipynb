{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Image Captioning Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries, Set Seeds, and Select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Third party packages\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, ViTImageProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COCO Mini Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "data_path = kagglehub.dataset_download(\n",
    "    \"nagasai524/mini-coco2014-dataset-for-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions(dataset_path):\n",
    "    \"\"\"Load image_id -> list of captions mapping from captions.json.\"\"\"\n",
    "    captions_path = os.path.join(dataset_path, \"captions.json\")\n",
    "    with open(captions_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    annotations = data[\"annotations\"] if isinstance(data, dict) else data\n",
    "\n",
    "    captions = {}\n",
    "    for ann in annotations:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        captions.setdefault(img_id, []).append(ann[\"caption\"])\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_folder(dataset_path, ext=\".jpg\"):\n",
    "    \"\"\"Find first folder under dataset_path that contains image files.\"\"\"\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        if any(f.lower().endswith(ext) for f in files):\n",
    "            return root\n",
    "    raise RuntimeError(f\"No image folder with *{ext} found under {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = load_captions(data_path)\n",
    "img_folder = find_image_folder(data_path)\n",
    "img_ids = sorted(captions.keys())\n",
    "\n",
    "print(f\"Dataset loaded, {len(img_ids)} images with captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Loading Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(idx):\n",
    "    \"\"\"Load image and its captions by index in img_ids.\"\"\"\n",
    "    img_id = img_ids[idx]\n",
    "\n",
    "    # Try common filename patterns\n",
    "    candidates = [\n",
    "        os.path.join(img_folder, f\"COCO_train2014_{img_id:012d}.jpg\"),\n",
    "        os.path.join(img_folder, f\"{img_id}.jpg\"),\n",
    "    ]\n",
    "\n",
    "    for img_path in candidates:\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            return img, captions[img_id]\n",
    "\n",
    "    raise FileNotFoundError(f\"No image file found for image_id {img_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspection of a Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an image to show\n",
    "idx = 6000\n",
    "\n",
    "img, caps = get_image(idx)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image index: {idx}, image_id: {img_ids[idx]}\")\n",
    "print(\"Ground truth captions:\")\n",
    "for cap in caps:\n",
    "    print(f\"  - {cap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ViT-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"eager\").to(device)\n",
    "\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "print(f\"Decoder attention implementation: {model.config.decoder._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption_and_attention(img, max_length=16):\n",
    "    \"\"\"\n",
    "    Generate a caption for a PIL image and return:\n",
    "    - caption (str)\n",
    "    - cross_attentions (list of tensors)\n",
    "    - encoder_hidden_states (tensor)\n",
    "    \"\"\"\n",
    "    # Image -> pixel_values\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Encode image\n",
    "    encoder_outputs = model.encoder(pixel_values=pixel_values)\n",
    "    encoder_hidden_states = encoder_outputs.last_hidden_state  # [1, 197, hidden_dim]\n",
    "\n",
    "    # Generate caption ids\n",
    "    generated_ids = model.generate(\n",
    "        pixel_values,\n",
    "        max_length=max_length,\n",
    "        num_beams=1)\n",
    "\n",
    "    # Run decoder once over full sequence to get cross-attentions\n",
    "    decoder_outputs = model.decoder(\n",
    "        input_ids=generated_ids,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        output_attentions=True,\n",
    "        return_dict=True,\n",
    "        use_cache=False)\n",
    "\n",
    "    caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return caption, decoder_outputs.cross_attentions, encoder_hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cross_attention(cross_attentions, img_size, patch_size=14):\n",
    "    \"\"\"\n",
    "    Aggregate cross-attention over heads and tokens into a 2D heatmap\n",
    "    in image resolution (sentence-based aggregation).\n",
    "    \n",
    "    cross_attentions: list of tensors, each [1, num_heads, tgt_len, src_len]\n",
    "                      src_len = 1 (CLS) + patch_size^2\n",
    "    img_size: (width, height) of the original image\n",
    "    \"\"\"\n",
    "    # Take last decoder layer (most informative)\n",
    "    last_layer = cross_attentions[-1]          # [1, num_heads, tgt_len, src_len]\n",
    "    last_layer = last_layer.squeeze(0)         # [num_heads, tgt_len, src_len]\n",
    "\n",
    "    # Average over heads and decoder tokens\n",
    "    avg_over_heads = last_layer.mean(dim=0)    # [tgt_len, src_len]\n",
    "    avg_over_tokens = avg_over_heads.mean(dim=0)  # [src_len]\n",
    "\n",
    "    # Drop CLS token and keep only patch tokens\n",
    "    patch_attention = avg_over_tokens[1:]      # [patch_size^2]\n",
    "\n",
    "    # Reshape to patch grid\n",
    "    attention_map = patch_attention.reshape(patch_size, patch_size).cpu().numpy()\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    attention_map -= attention_map.min()\n",
    "    denom = attention_map.max() if attention_map.max() > 0 else 1e-8\n",
    "    attention_map /= denom\n",
    "\n",
    "    # Upsample to original image size\n",
    "    w, h = img_size\n",
    "    attn_img = Image.fromarray((attention_map * 255).astype(np.uint8))\n",
    "    attn_resized = attn_img.resize((w, h), resample=Image.BICUBIC)\n",
    "\n",
    "    return np.array(attn_resized) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_attention(img, caption, attention_heatmap):\n",
    "    \"\"\"Plot original image, attention heatmap, and overlay.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # 1) Original image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Original image\\nCaption: '{caption}'\", fontsize=10)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # 2) Heatmap only\n",
    "    axes[1].imshow(attention_heatmap, cmap=\"hot\")\n",
    "    axes[1].set_title(\"Cross-attention heatmap\\n(sentence-based)\", fontsize=10)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # 3) Overlay\n",
    "    axes[2].imshow(img)\n",
    "    axes[2].imshow(attention_heatmap, cmap=\"hot\", alpha=0.5)\n",
    "    axes[2].set_title(\"Attention overlay\\n(red = high attention)\", fontsize=10)\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Pick an Image, Get Caption and Attention and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt_caps = get_image(idx)\n",
    "\n",
    "caption, cross_attentions, _ = generate_caption_and_attention(img)\n",
    "attention_heatmap = aggregate_cross_attention(cross_attentions, img.size)\n",
    "\n",
    "show_image_and_attention(img, caption, attention_heatmap)\n",
    "\n",
    "print(f\"Image index: {idx}, image_id: {img_ids[idx]}\")\n",
    "print(\"\\nGround-truth captions:\")\n",
    "for cap in gt_caps:\n",
    "    print(f\"  - {cap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Candidate Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_candidate_pixels(attention_heatmap, k):\n",
    "    \"\"\"\n",
    "    Select top-k pixels with highest attention as candidate region.\n",
    "    Returns list of (y, x) indices.\n",
    "    \"\"\"\n",
    "    flat = attention_heatmap.flatten()\n",
    "    k = min(k, flat.size)\n",
    "    idxs = np.argpartition(flat, -k)[-k:]\n",
    "    ys, xs = np.unravel_index(idxs, attention_heatmap.shape)\n",
    "    return list(zip(ys, xs))\n",
    "\n",
    "\n",
    "def show_candidate_region(img, attention_heatmap, candidate_pixels):\n",
    "    \"\"\"\n",
    "    Visualize original image and candidate region (pixels we may perturb).\n",
    "    \"\"\"\n",
    "    mask = np.zeros(attention_heatmap.shape, dtype=bool)\n",
    "    for y, x in candidate_pixels:\n",
    "        mask[y, x] = True\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Original image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Original image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Candidate overlay\n",
    "    axes[1].imshow(img)\n",
    "    axes[1].imshow(mask, cmap=\"Reds\", alpha=0.6)\n",
    "    axes[1].set_title(\"Candidate region (pixels we may perturb)\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_IDX = idx\n",
    "\n",
    "# Image and Ground Truth Captions\n",
    "base_img, gt_caps = get_image(ATTACK_IDX)\n",
    "\n",
    "# 2) Caption + Cross-Attention vom Modell holen\n",
    "base_caption, cross_attentions, _ = generate_caption_and_attention(base_img)\n",
    "\n",
    "# 3) Attention-Heatmap aggregieren\n",
    "attention_heatmap = aggregate_cross_attention(cross_attentions, base_img.size)\n",
    "\n",
    "# 4) Top-k Pixel als Candidate Region wÃ¤hlen\n",
    "candidate_pixels = get_topk_candidate_pixels(attention_heatmap, k=35000)\n",
    "\n",
    "print(f\"Attack index: {ATTACK_IDX}\")\n",
    "print(f\"Base caption: '{base_caption}'\")\n",
    "print(f\"Number of candidate pixels: {len(candidate_pixels)}\")\n",
    "\n",
    "# 5) Candidate Region visualisieren\n",
    "show_candidate_region(base_img, attention_heatmap, candidate_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(img):\n",
    "    \"\"\"Convert PIL image to uint8 numpy array of shape (H, W, 3).\"\"\"\n",
    "    return np.array(img).astype(np.uint8)\n",
    "\n",
    "\n",
    "def array_to_image(arr):\n",
    "    \"\"\"Convert numpy array (H, W, 3) back to PIL image.\"\"\"\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "\n",
    "def apply_perturbation(base_array, delta_vec, candidate_pixels):\n",
    "    \"\"\"\n",
    "    Apply a perturbation vector to the candidate pixels of an image.\n",
    "    \n",
    "    base_array: numpy array (H, W, 3), uint8\n",
    "    delta_vec: 1D numpy array of length 3 * len(candidate_pixels)\n",
    "               [dR1, dG1, dB1, dR2, dG2, dB2, ...]\n",
    "    candidate_pixels: list of (y, x) positions\n",
    "    \"\"\"\n",
    "    H, W, C = base_array.shape\n",
    "    assert C == 3, \"Expected RGB image\"\n",
    "    num_pixels = len(candidate_pixels)\n",
    "    assert delta_vec.shape[0] == 3 * num_pixels, \"delta_vec length mismatch\"\n",
    "\n",
    "    # work on a float copy so we can add positive/negative deltas\n",
    "    perturbed = base_array.astype(np.float32).copy()\n",
    "    deltas = delta_vec.reshape(num_pixels, 3)\n",
    "\n",
    "    for (y, x), d in zip(candidate_pixels, deltas):\n",
    "        # add deltas to this pixel (R,G,B)\n",
    "        perturbed[y, x, :] += d\n",
    "\n",
    "    # clip back to valid image range\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return array_to_image(perturbed)\n",
    "\n",
    "\n",
    "# base image as numpy array (used for all perturbations)\n",
    "base_array = image_to_array(base_img)\n",
    "\n",
    "test_img = apply_perturbation(base_array, test_delta, candidate_pixels)\n",
    "plt.imshow(test_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption(img, max_length=16):\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values, max_length=max_length, num_beams=1)\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Optional sanity check: make sure base_caption matches the current generate() config\n",
    "print(\"Base caption (from attention step):\", base_caption)\n",
    "print(\"Base caption (fresh generate)     :\", generate_caption(base_img))\n",
    "\n",
    "\n",
    "# Prepare token IDs of the base caption (this is the fixed target for the loss)\n",
    "base_ids = tokenizer(\n",
    "    base_caption,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    ").input_ids.to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def caption_loss_for_image(img):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss of the fixed base caption given the image.\n",
    "    Higher loss = caption less compatible with the image.\n",
    "    \"\"\"\n",
    "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    outputs = model(pixel_values=pixel_values, labels=base_ids)\n",
    "    return outputs.loss.item()\n",
    "\n",
    "\n",
    "# Loss for the original (unperturbed) image\n",
    "base_loss = caption_loss_for_image(base_img)\n",
    "print(f\"Base loss for original image: {base_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_REG = 0.005  # regularization strength for perturbation size\n",
    "\n",
    "def fitness(x):\n",
    "    \"\"\"\n",
    "    x: 1D numpy array with length 3 * len(candidate_pixels)\n",
    "    returns: scalar, higher = better attack but penalized for large perturbations\n",
    "    \"\"\"\n",
    "    perturbed_img = apply_perturbation(base_array, x, candidate_pixels)\n",
    "    loss = caption_loss_for_image(perturbed_img)\n",
    "    loss_increase = loss - base_loss  # how much worse the caption fits\n",
    "\n",
    "    # L2 norm of the perturbation (normalized)\n",
    "    norm = np.linalg.norm(x) / (255.0 * np.sqrt(x.size))\n",
    "\n",
    "    # We want high loss_increase but small norm\n",
    "    return loss_increase - LAMBDA_REG * norm\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "num_pixels = len(candidate_pixels)\n",
    "dim = 3 * num_pixels\n",
    "\n",
    "print(f\"Number of candidate pixels: {num_pixels}\")\n",
    "print(f\"Dimension of perturbation vector: {dim}\")\n",
    "\n",
    "x_zero = np.zeros(dim, dtype=np.float32)\n",
    "score_zero = fitness(x_zero)\n",
    "print(f\"Fitness(x_zero): {score_zero:.4f}\")\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "eps = 20.0\n",
    "x_rand = rng.uniform(low=-eps, high=eps, size=dim).astype(np.float32)\n",
    "score_rand = fitness(x_rand)\n",
    "print(f\"Fitness(x_rand): {score_rand:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_evolution(\n",
    "    fitness_fn,\n",
    "    dim,\n",
    "    pop_size=8,\n",
    "    generations=5,\n",
    "    F=0.5,\n",
    "    CR=0.7,\n",
    "    eps=20.0,\n",
    "    random_seed=42):\n",
    "    \"\"\"\n",
    "    Very simple Differential Evolution (DE/rand/1/bin).\n",
    "    \n",
    "    fitness_fn: function x -> scalar (higher = better)\n",
    "    dim: dimensionality of x (here: 3 * len(candidate_pixels))\n",
    "    pop_size: number of individuals in the population\n",
    "    generations: number of DE generations\n",
    "    F: mutation factor\n",
    "    CR: crossover rate\n",
    "    eps: initial range for parameters: x ~ U[-eps, +eps]\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "\n",
    "    # 1) Initialize population with random vectors\n",
    "    pop = rng.uniform(low=-eps, high=eps, size=(pop_size, dim)).astype(np.float32)\n",
    "    fitness_vals = np.array([fitness_fn(ind) for ind in pop], dtype=np.float32)\n",
    "\n",
    "    best_idx = int(np.argmax(fitness_vals))\n",
    "    best_x = pop[best_idx].copy()\n",
    "    best_f = float(fitness_vals[best_idx])\n",
    "\n",
    "    print(f\"Initial best fitness: {best_f:.4f}\")\n",
    "\n",
    "    # 2) Iterate over generations\n",
    "    for gen in range(generations):\n",
    "        print(f\"\\n--- Generation {gen + 1}/{generations} ---\")\n",
    "\n",
    "        for i in range(pop_size):\n",
    "            # Choose indices for mutation (a, b, c all different and all != i)\n",
    "            idxs = list(range(pop_size))\n",
    "            idxs.remove(i)\n",
    "            a, b, c = rng.choice(idxs, size=3, replace=False)\n",
    "\n",
    "            x_a, x_b, x_c = pop[a], pop[b], pop[c]\n",
    "\n",
    "            # Mutation: v = x_a + F * (x_b - x_c)\n",
    "            v = x_a + F * (x_b - x_c)\n",
    "\n",
    "            # Crossover: binomial\n",
    "            cross_mask = rng.random(dim) < CR\n",
    "            # Ensure at least one component comes from v\n",
    "            j_rand = rng.integers(0, dim)\n",
    "            cross_mask[j_rand] = True\n",
    "\n",
    "            u = np.where(cross_mask, v, pop[i])\n",
    "\n",
    "            # Evaluate trial vector\n",
    "            f_u = fitness_fn(u)\n",
    "\n",
    "            # Selection: keep trial if it is better\n",
    "            if f_u > fitness_vals[i]:\n",
    "                pop[i] = u\n",
    "                fitness_vals[i] = f_u\n",
    "\n",
    "                # Update global best if necessary\n",
    "                if f_u > best_f:\n",
    "                    best_f = float(f_u)\n",
    "                    best_x = u.copy()\n",
    "\n",
    "        print(f\"Best fitness after generation {gen + 1}: {best_f:.4f}\")\n",
    "\n",
    "    return best_x, best_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the search space\n",
    "dim = 3 * len(candidate_pixels)\n",
    "print(f\"Dimensionality of x: {dim}\")\n",
    "\n",
    "best_x, best_f = differential_evolution(\n",
    "    fitness_fn=fitness,\n",
    "    dim=dim,\n",
    "    pop_size=6,      # keep small because each call runs the model\n",
    "    generations=3,   # small number of iterations for testing\n",
    "    F=0.6,\n",
    "    CR=0.7,\n",
    "    eps=20.0,       # allow strong per-channel perturbations\n",
    ")\n",
    "\n",
    "print(\"\\n=== Differential Evolution finished ===\")\n",
    "print(f\"Best fitness found (loss increase): {best_f:.4f}\")\n",
    "\n",
    "# Create best adversarial image from the found perturbation\n",
    "best_img = apply_perturbation(base_array, best_x, candidate_pixels)\n",
    "best_caption = generate_caption(best_img)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(base_img)\n",
    "plt.title(f\"Original\\n'{base_caption}'\", fontsize=9)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(best_img)\n",
    "plt.title(f\"Adversarial\\n'{best_caption}'\", fontsize=9)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOriginal caption : '{base_caption}'\")\n",
    "print(f\"Adversarial cap. : '{best_caption}'\")\n",
    "print(f\"Base loss        : {base_loss:.4f}\")\n",
    "print(f\"Best loss (base+f): {base_loss + best_f:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
