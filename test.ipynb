{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7531cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"nagasai524/mini-coco2014-dataset-for-image-captioning\")\n",
    "print(f\"Path to dataset files: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path\n",
    "json_file = os.path.join(dataset_path, \"captions.json\")\n",
    "\n",
    "with open(json_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "annotations = data[\"annotations\"] if isinstance(data, dict) and \"annotations\" in data else data\n",
    "\n",
    "captions_dict = {}\n",
    "for ann in annotations:\n",
    "    img_id = ann[\"image_id\"]\n",
    "    if img_id not in captions_dict:\n",
    "        captions_dict[img_id] = []\n",
    "    captions_dict[img_id].append(ann[\"caption\"])\n",
    "\n",
    "image_dir = None\n",
    "for root, dirs, files in os.walk(path):\n",
    "    if any(f.endswith(\".jpg\") for f in files):\n",
    "        image_dir = root\n",
    "        break\n",
    "\n",
    "print(f\"Loaded {len(captions_dict)} images with captions\")\n",
    "print(f\"Image directory: {image_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_image(image_id, size=384):\n",
    "    img_path = os.path.join(image_dir, f\"COCO_train2014_{image_id:012d}.jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        img_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "    \n",
    "    img = Image.open(img_path).convert(\"RGB\").resize((size, size))\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_coco_dataset(id=3, for_attention=False):\n",
    "    image_ids = list(captions_dict.keys())\n",
    "    img_id = image_ids[id]\n",
    "    \n",
    "    img = load_coco_image(img_id, size=384)\n",
    "    captions = captions_dict[img_id]\n",
    "    \n",
    "    if for_attention:\n",
    "        transform = transforms.ToTensor()\n",
    "        return transform(img)\n",
    "    else:\n",
    "        return None, captions, img\n",
    "\n",
    "\n",
    "def divide_list(lst, num_chunks=5):\n",
    "    chunk_size = len(lst) // num_chunks\n",
    "    remainder = len(lst) % num_chunks\n",
    "    result = []\n",
    "    start = 0\n",
    "    for i in range(num_chunks):\n",
    "        end = start + chunk_size\n",
    "        if remainder > 0:\n",
    "            end += 1\n",
    "            remainder -= 1\n",
    "        result.append(lst[start:end])\n",
    "        start = end\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_image(id, image_size=384, device=device, before=True, dataset=\"coco\"):\n",
    "    if dataset != \"coco\":\n",
    "        raise ValueError(f\"Dataset {dataset} not supported\")\n",
    "    \n",
    "    _, _, raw_image = get_coco_dataset(id)\n",
    "    \n",
    "    transform_pil_tensor = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.PILToTensor(),\n",
    "    ])\n",
    "\n",
    "    transform_tensor = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.48145466, 0.4578275, 0.40821073), \n",
    "            (0.26862954, 0.26130258, 0.27577711)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    if before:\n",
    "        image = transform_pil_tensor(raw_image).to(device) \n",
    "    else:\n",
    "        image = transform_tensor(raw_image).unsqueeze(0).to(device) \n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, captions, img = get_coco_dataset(id=0)\n",
    "print(f\"Image type: {type(img)}, Size: {img.size}\")\n",
    "print(f\"First caption: {captions[0]}\")\n",
    "\n",
    "test_list = list(range(25))\n",
    "divided = divide_list(test_list, num_chunks=5)\n",
    "print(f\"divide_list test: {len(divided)} chunks\")\n",
    "\n",
    "tensor_img = load_image(id=0, image_size=384, device=device, before=True)\n",
    "print(f\"load_image output shape: {tensor_img.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
