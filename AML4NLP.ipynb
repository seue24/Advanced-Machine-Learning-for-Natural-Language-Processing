{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import kagglehub\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"nagasai524/mini-coco2014-dataset-for-image-captioning\")\n",
    "\n",
    "with open(os.path.join(path, \"captions.json\"), \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    annotations = data[\"annotations\"] if isinstance(data, dict) else data\n",
    "\n",
    "captions = {}\n",
    "for item in annotations:\n",
    "    img_id = item[\"image_id\"]\n",
    "    if img_id not in captions:\n",
    "        captions[img_id] = []\n",
    "    captions[img_id].append(item[\"caption\"])\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    if any(f.endswith(\".jpg\") for f in files):\n",
    "        img_folder = root\n",
    "        break\n",
    "\n",
    "img_ids = list(captions.keys())\n",
    "print(f\"Ready: {len(captions)} images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(idx):\n",
    "    img_id = img_ids[idx]\n",
    "    img_path = os.path.join(img_folder, f\"COCO_train2014_{img_id:012d}.jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        img_path = os.path.join(img_folder, f\"{img_id}.jpg\")\n",
    "    return Image.open(img_path).convert(\"RGB\").resize((384, 384)), captions[img_id]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img, caps = get_image(i)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{caps[0][:50]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [0, 1, 2]\n",
    "\n",
    "for idx in selected:\n",
    "    img, caps = get_image(idx)\n",
    "    print(f\"Image {idx}: {caps[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.48145466, 0.4578275, 0.40821073], \n",
    "                        [0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "def to_tensor(idx):\n",
    "    img, _ = get_image(idx)\n",
    "    return normalize(img).unsqueeze(0).to(device)\n",
    "\n",
    "test = to_tensor(0)\n",
    "print(f\"Tensor ready for BLIP: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load BLIP model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
    "model.eval()\n",
    "print(\"BLIP model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hallo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
